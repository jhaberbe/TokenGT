{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "841eb738",
   "metadata": {},
   "source": [
    "# Reading the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "287126a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jameshaberberger/TokenGT/.venv/lib/python3.13/site-packages/anndata/_core/storage.py:39: ImplicitModificationWarning: X should not be a np.matrix, use np.ndarray instead.\n",
      "  warnings.warn(msg, ImplicitModificationWarning)\n"
     ]
    }
   ],
   "source": [
    "import scanpy as sc\n",
    "adata = sc.read_h5ad(\"../data/05-27.h5ad\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83d99e15",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jameshaberberger/TokenGT/.venv/lib/python3.13/site-packages/legacy_api_wrap/__init__.py:82: UserWarning: Some cells have zero counts\n",
      "  return fn(*args_all, **kw)\n"
     ]
    }
   ],
   "source": [
    "# Convert sparse matrix to dense if necessary\n",
    "adata.X = adata.X.todense()\n",
    "adata.layers[\"counts\"] = adata.X.copy()\n",
    "\n",
    "# Normalize and log-transform the data\n",
    "sc.pp.normalize_total(adata)\n",
    "sc.pp.log1p(adata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "6eb649c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "adata.obs[\"batch\"] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5035cffb",
   "metadata": {},
   "source": [
    "# Spatial Dataset Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "99c3fab6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/2n/j06nrn2n7r524t776sngh0xr0000gr/T/ipykernel_88207/1915100172.py:22: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  batch = torch.tensor(self.adata.obs[\"batch\"]),\n",
      "/var/folders/2n/j06nrn2n7r524t776sngh0xr0000gr/T/ipykernel_88207/1915100172.py:28: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  plin2 = torch.tensor(self.adata.obs[\"plin2_area\"]).float(),\n",
      "/var/folders/2n/j06nrn2n7r524t776sngh0xr0000gr/T/ipykernel_88207/1915100172.py:29: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  oro = torch.tensor(self.adata.obs[\"oil_red_o_area\"]).float(),\n",
      "/var/folders/2n/j06nrn2n7r524t776sngh0xr0000gr/T/ipykernel_88207/1915100172.py:30: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  lipid_droplet = torch.tensor(self.adata.obs[\"lipid_droplet_area\"]).float(),\n",
      "/var/folders/2n/j06nrn2n7r524t776sngh0xr0000gr/T/ipykernel_88207/1915100172.py:33: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  distance_to_nearest_amyloid = torch.tensor(self.adata.obs[\"lipid_droplet_area\"]).float()\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch_geometric\n",
    "from torch_geometric.data import Data, Dataset\n",
    "\n",
    "import numpy as np\n",
    "from scipy.spatial import cKDTree\n",
    "\n",
    "class GraphDatasetGenerator:\n",
    "    def __init__(self, adata):\n",
    "        self.adata = adata\n",
    "\n",
    "    def generate_graph_data(self):\n",
    "        # Edge index\n",
    "        edge_index = self.create_edge_index()\n",
    "\n",
    "        data = Data(\n",
    "            # Expression\n",
    "            x=torch.tensor(self.adata.X).float(), \n",
    "            counts = torch.tensor(self.adata.layers[\"counts\"]).float(),\n",
    "\n",
    "            # Batch information\n",
    "            batch = torch.tensor(self.adata.obs[\"batch\"]),\n",
    "\n",
    "            # Spatial \n",
    "            edge_index=edge_index,\n",
    "\n",
    "            # Pathology\n",
    "            plin2 = torch.tensor(self.adata.obs[\"plin2_area\"]).float(),\n",
    "            oro = torch.tensor(self.adata.obs[\"oil_red_o_area\"]).float(),\n",
    "            lipid_droplet = torch.tensor(self.adata.obs[\"lipid_droplet_area\"]).float(),\n",
    "\n",
    "            # Distance to nearest amyloid\n",
    "            distance_to_nearest_amyloid = torch.tensor(self.adata.obs[\"lipid_droplet_area\"]).float()\n",
    "        )\n",
    "\n",
    "        return data\n",
    "\n",
    "    def create_edge_index(self):\n",
    "        tree = cKDTree(\n",
    "            self.adata.obs[['x_centroid', 'y_centroid']].values\n",
    "        )\n",
    "\n",
    "        _, neighbors = tree.query(\n",
    "            self.adata.obs[['x_centroid', 'y_centroid']].values, \n",
    "            k=31\n",
    "        )\n",
    "\n",
    "        rows = np.repeat(\n",
    "            np.arange(\n",
    "                len(adata.obs[['x_centroid', 'y_centroid']].values)\n",
    "            ), \n",
    "            30\n",
    "        )\n",
    "        cols = neighbors[:, 1:].reshape(-1)\n",
    "        return torch.tensor([rows, cols], dtype=torch.long)\n",
    "\n",
    "data = GraphDatasetGenerator(adata).generate_graph_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c22bbe2e",
   "metadata": {},
   "source": [
    "# Constructing our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "918bd0ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([69552, 96])"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "class EmbeddingEncoder(nn.Module):\n",
    "    def __init__(self, input_dim, num_batches):\n",
    "        super(EmbeddingEncoder, self).__init__()\n",
    "\n",
    "        # Expression encoder\n",
    "        self.expression_encoder = Encoder(input_dim)\n",
    "\n",
    "        # Pathology heads\n",
    "        def mlp():\n",
    "            return nn.Sequential(\n",
    "                nn.Linear(1, 64),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(64, 8)\n",
    "            )\n",
    "\n",
    "        self.pathology_head = nn.ModuleDict({\n",
    "            \"oil_red_o\": mlp(),\n",
    "            \"plin2\": mlp(),\n",
    "            \"lipid_droplet\": mlp(),\n",
    "            \"distance_to_amyloid\": mlp()\n",
    "        })\n",
    "\n",
    "    def forward(self, data):\n",
    "        # Encode expression\n",
    "        expression = self.expression_encoder(data.x)\n",
    "\n",
    "        # Encode pathology\n",
    "        pathology = torch.cat([\n",
    "            self.pathology_head[\"oil_red_o\"](data.oro.unsqueeze(1)),\n",
    "            self.pathology_head[\"plin2\"](data.plin2.unsqueeze(1)),\n",
    "            self.pathology_head[\"lipid_droplet\"](data.lipid_droplet.unsqueeze(1)),\n",
    "            self.pathology_head[\"distance_to_amyloid\"](data.distance_to_nearest_amyloid.unsqueeze(1)),\n",
    "        ], dim=1)\n",
    "\n",
    "        # Concatenate all embeddings\n",
    "        full_embedding = torch.cat([expression, pathology], dim=1)\n",
    "        return full_embedding\n",
    "\n",
    "model = EmbeddingEncoder(\n",
    "    input_dim=data.x.shape[1], \n",
    "    num_batches=adata.obs[\"batch\"].nunique()\n",
    ")\n",
    "output = model(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8fb5a2d",
   "metadata": {},
   "source": [
    "# Getting to go"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11c3b5af",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3a94b72",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 460/69552 [00:03<08:50, 130.29it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[220]\u001b[39m\u001b[32m, line 165\u001b[39m\n\u001b[32m    163\u001b[39m example = []\n\u001b[32m    164\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m center \u001b[38;5;129;01min\u001b[39;00m tqdm(torch.randperm(data.num_nodes)):\n\u001b[32m--> \u001b[39m\u001b[32m165\u001b[39m     sub = \u001b[43mget_subgraph\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcenter_nodes\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43mcenter\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    166\u001b[39m     out = model(sub.to(device))[sub.node_mapping[\u001b[32m0\u001b[39m]]\n\u001b[32m    168\u001b[39m     example.append(mtoh(out))\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[220]\u001b[39m\u001b[32m, line 140\u001b[39m, in \u001b[36mget_subgraph\u001b[39m\u001b[34m(data, center_nodes, num_hops)\u001b[39m\n\u001b[32m    139\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget_subgraph\u001b[39m(data, center_nodes, num_hops=\u001b[32m1\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m140\u001b[39m     subset, edge_index, mapping, edge_mask = \u001b[43mk_hop_subgraph\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    141\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnode_idx\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcenter_nodes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    142\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnum_hops\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_hops\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    143\u001b[39m \u001b[43m        \u001b[49m\u001b[43medge_index\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m.\u001b[49m\u001b[43medge_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    144\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrelabel_nodes\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[32m    145\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    146\u001b[39m     sub_data = data.subgraph(subset)\n\u001b[32m    147\u001b[39m     sub_data.edge_index = edge_index\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/TokenGT/.venv/lib/python3.13/site-packages/torch_geometric/utils/_subgraph.py:353\u001b[39m, in \u001b[36mk_hop_subgraph\u001b[39m\u001b[34m(node_idx, num_hops, edge_index, relabel_nodes, num_nodes, flow, directed)\u001b[39m\n\u001b[32m    351\u001b[39m     node_mask[subsets[-\u001b[32m1\u001b[39m]] = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    352\u001b[39m     torch.index_select(node_mask, \u001b[32m0\u001b[39m, row, out=edge_mask)\n\u001b[32m--> \u001b[39m\u001b[32m353\u001b[39m     subsets.append(col[edge_mask])\n\u001b[32m    355\u001b[39m subset, inv = torch.cat(subsets).unique(return_inverse=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m    356\u001b[39m inv = inv[:node_idx.numel()]\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
    "from torch_geometric.utils import to_dense_batch\n",
    "\n",
    "class TokenGT(nn.Module):\n",
    "    def __init__(self, input_dim, num_batches, num_nodes, d_model=128, nhead=8, num_layers=4):\n",
    "        super(TokenGT, self).__init__()\n",
    "        self.d_model = d_model\n",
    "\n",
    "        # === Node Encoder ===\n",
    "        self.node_encoder = EmbeddingEncoder(input_dim, num_batches)\n",
    "\n",
    "        # === Edge Encoder ===\n",
    "        self.edge_encoder = nn.Sequential(\n",
    "            nn.Linear(2 * d_model, d_model),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(d_model, d_model)\n",
    "        )\n",
    "\n",
    "        # === Type Embedding: 0=node, 1=edge, 2=graph token ===\n",
    "        self.type_embedding = nn.Embedding(3, d_model)\n",
    "\n",
    "        # === Position (Node ID) Embedding ===\n",
    "        self.position_embedding = nn.Embedding(num_nodes, d_model)\n",
    "\n",
    "        # === [GRAPH] token ===\n",
    "        self.graph_token = nn.Parameter(torch.randn(1, 1, d_model))\n",
    "\n",
    "        # === Transformer Encoder ===\n",
    "        encoder_layer = TransformerEncoderLayer(d_model=d_model, nhead=nhead, batch_first=True)\n",
    "        self.transformer = TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "\n",
    "        # === Prediction Head ===\n",
    "        self.output_head = nn.Sequential(\n",
    "            nn.Linear(d_model, d_model),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(d_model, 1)  # Adjust output dimension as needed\n",
    "        )\n",
    "\n",
    "    def forward(self, data):\n",
    "        B = data.x.shape[0]\n",
    "        N = data.x.size(0)\n",
    "        E = data.edge_index.size(1)\n",
    "\n",
    "        # === Node Tokens ===\n",
    "        node_embed = self.node_encoder(data)  # (N, d_model)\n",
    "\n",
    "        # === Edge Tokens ===\n",
    "        src = data.edge_index[0]\n",
    "        dst = data.edge_index[1]\n",
    "        edge_embed = self.edge_encoder(torch.cat([node_embed[src], node_embed[dst]], dim=1))  # (E, d_model)\n",
    "\n",
    "        # === Type and Position Embedding ===\n",
    "        node_type = self.type_embedding(torch.zeros(N, dtype=torch.long, device=node_embed.device))  # type 0\n",
    "        edge_type = self.type_embedding(torch.ones(E, dtype=torch.long, device=node_embed.device))   # type 1\n",
    "        node_pos = self.position_embedding(data.node_id if hasattr(data, \"node_id\") else torch.arange(N, device=node_embed.device))\n",
    "\n",
    "        node_tokens = node_embed + node_type + node_pos\n",
    "        edge_tokens = edge_embed + edge_type\n",
    "\n",
    "        # === [GRAPH] Token ===\n",
    "        graph_token = self.graph_token.expand(B, -1, -1)  # (B, 1, d_model)\n",
    "        \n",
    "        # === Batch Assembly ===\n",
    "        tokens = torch.cat([node_tokens, edge_tokens], dim=0)  # (N + E, d_model)\n",
    "        batch_vec = torch.cat([data.batch, data.batch[src]])   # Match token order to batch size\n",
    "\n",
    "        # === Dense Batch for Transformer ===\n",
    "        token_batch, mask = to_dense_batch(tokens, batch_vec)  # (B, T, d_model)\n",
    "\n",
    "        # Prepend [GRAPH] token\n",
    "        token_batch = torch.cat([graph_token, token_batch], dim=1)  # (B, T+1, d_model)\n",
    "        mask = torch.cat([torch.ones((B, 1), dtype=torch.bool, device=mask.device), mask], dim=1)\n",
    "\n",
    "        # === Transformer ===\n",
    "        out = self.transformer(token_batch, src_key_padding_mask=~mask)  # (B, T+1, d_model)\n",
    "\n",
    "        # === Predict from [GRAPH] token ===\n",
    "        graph_repr = out[:, 0]  # (B, d_model)\n",
    "        return self.output_head(graph_repr)\n",
    "\n",
    "\n",
    "class MultiTaskOutputHeads(nn.Module):\n",
    "    def __init__(self, in_dim, num_pathologies, num_nb_outputs=1, per_output_dispersion=False):\n",
    "        super().__init__()\n",
    "\n",
    "        self.num_nb_outputs = num_nb_outputs\n",
    "\n",
    "        # --- Count prediction (Negative Binomial) ---\n",
    "        self.count_mu_head = nn.Linear(in_dim, num_nb_outputs)  # predicts log_mu: (B, num_nb_outputs)\n",
    "\n",
    "        if per_output_dispersion:\n",
    "            self.log_dispersion = nn.Parameter(torch.zeros(num_nb_outputs))  # separate for each output\n",
    "        else:\n",
    "            self.log_dispersion = nn.Parameter(torch.tensor(0.0))  # shared\n",
    "\n",
    "        # --- Hurdle Log-Normal for pathologies ---\n",
    "        self.hurdle_zero_logits = nn.Linear(in_dim, num_pathologies)\n",
    "        self.hurdle_log_mu = nn.Linear(in_dim, num_pathologies)\n",
    "        self.hurdle_log_sigma = nn.Linear(in_dim, num_pathologies)\n",
    "\n",
    "        # --- Binary classification: proximity to amyloid ---\n",
    "        self.amyloid_head = nn.Linear(in_dim, 1)\n",
    "\n",
    "    def forward(self, center_token):\n",
    "        # --- NB count ---\n",
    "        log_mu = self.count_mu_head(center_token)  # (B, num_nb_outputs)\n",
    "        if self.log_dispersion.ndim == 0:\n",
    "            log_disp = self.log_dispersion.expand_as(log_mu)  # scalar -> (B, K)\n",
    "        else:\n",
    "            log_disp = self.log_dispersion.unsqueeze(0).expand(center_token.size(0), -1)  # (B, K)\n",
    "\n",
    "        # --- Hurdle Log-Normal ---\n",
    "        zero_logits = self.hurdle_zero_logits(center_token)\n",
    "        hln_log_mu = self.hurdle_log_mu(center_token)\n",
    "        hln_log_sigma = self.hurdle_log_sigma(center_token)\n",
    "\n",
    "        # --- Binary prediction ---\n",
    "        amyloid_logit = self.amyloid_head(center_token)\n",
    "\n",
    "        return {\n",
    "            \"nb\": {\n",
    "                \"log_mu\": log_mu,\n",
    "                \"log_dispersion\": log_disp\n",
    "            },\n",
    "            \"hurdle_lognorm\": {\n",
    "                \"zero_logits\": zero_logits,\n",
    "                \"log_mu\": hln_log_mu,\n",
    "                \"log_sigma\": hln_log_sigma\n",
    "            },\n",
    "            \"amyloid\": {\n",
    "                \"logits\": amyloid_logit\n",
    "            }\n",
    "        }\n",
    "\n",
    "from torch_geometric.utils import k_hop_subgraph\n",
    "\n",
    "def get_subgraph(data, center_nodes, num_hops=1):\n",
    "    subset, edge_index, mapping, edge_mask = k_hop_subgraph(\n",
    "        node_idx=center_nodes,\n",
    "        num_hops=num_hops,\n",
    "        edge_index=data.edge_index,\n",
    "        relabel_nodes=True\n",
    "    )\n",
    "    sub_data = data.subgraph(subset)\n",
    "    sub_data.edge_index = edge_index\n",
    "    sub_data.edge_mask = edge_mask\n",
    "    sub_data.node_mapping = mapping\n",
    "    return sub_data\n",
    "\n",
    "token_gt = TokenGT(data.x.shape[1], \n",
    "        adata.obs[\"batch\"].nunique(), \n",
    "        data.x.shape[0],\n",
    "        d_model=96, \n",
    "        nhead=8, \n",
    "        num_layers=4)\n",
    "\n",
    "\n",
    "batch_size = 1\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "example = []\n",
    "for center in tqdm(torch.randperm(data.num_nodes)):\n",
    "    sub = get_subgraph(data, center_nodes=[center])\n",
    "    out = model(sub.to(device))[sub.node_mapping[0]]\n",
    "    example.append(mtoh(out))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tokengt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
