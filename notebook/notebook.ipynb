{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "841eb738",
   "metadata": {},
   "source": [
    "# Reading the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "287126a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jhaberbe/Projects/Personal/TokenGT/.venv/lib/python3.13/site-packages/anndata/_core/anndata.py:1756: UserWarning: Observation names are not unique. To make them unique, call `.obs_names_make_unique`.\n",
      "  utils.warn_names_duplicates(\"obs\")\n",
      "/home/jhaberbe/Projects/Personal/TokenGT/.venv/lib/python3.13/site-packages/anndata/_core/storage.py:39: ImplicitModificationWarning: Layer 'counts' should not be a np.matrix, use np.ndarray instead.\n",
      "  warnings.warn(msg, ImplicitModificationWarning)\n"
     ]
    }
   ],
   "source": [
    "import scanpy as sc\n",
    "adata = sc.read_h5ad(\"../data/output-dgi-10-10-20MAY2025.h5ad\")\n",
    "adata.layers[\"counts\"] = adata.layers[\"counts\"].todense()\n",
    "adata = adata[adata.obs[\"folder\"].eq(\"05-27\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5035cffb",
   "metadata": {},
   "source": [
    "# Spatial Dataset Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "99c3fab6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3253232/1310847928.py:22: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  batch = torch.tensor(self.adata.obs[\"folder\"].cat.codes),\n",
      "/tmp/ipykernel_3253232/1310847928.py:28: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  plin2 = torch.tensor(self.adata.obs[\"plin2_area\"]).float(),\n",
      "/tmp/ipykernel_3253232/1310847928.py:29: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  oro = torch.tensor(self.adata.obs[\"oil_red_o_area\"]).float(),\n",
      "/tmp/ipykernel_3253232/1310847928.py:30: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  lipid_droplet = torch.tensor(self.adata.obs[\"lipid_droplet_area\"]).float(),\n",
      "/tmp/ipykernel_3253232/1310847928.py:33: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  distance_to_nearest_amyloid = torch.tensor(self.adata.obs[\"lipid_droplet_area\"]).float()\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch_geometric\n",
    "from torch_geometric.data import Data, Dataset\n",
    "\n",
    "import numpy as np\n",
    "from scipy.spatial import cKDTree\n",
    "\n",
    "class GraphDatasetGenerator:\n",
    "    def __init__(self, adata):\n",
    "        self.adata = adata\n",
    "\n",
    "    def generate_graph_data(self):\n",
    "        # Edge index\n",
    "        edge_index = self.create_edge_index()\n",
    "\n",
    "        data = Data(\n",
    "            # Expression\n",
    "            x=torch.tensor(self.adata.X).float(), \n",
    "            counts = torch.tensor(self.adata.layers[\"counts\"]).float(),\n",
    "\n",
    "            # Batch information\n",
    "            batch = torch.tensor(self.adata.obs[\"folder\"].cat.codes),\n",
    "\n",
    "            # Spatial \n",
    "            edge_index=edge_index,\n",
    "\n",
    "            # Pathology\n",
    "            plin2 = torch.tensor(self.adata.obs[\"plin2_area\"]).float(),\n",
    "            oro = torch.tensor(self.adata.obs[\"oil_red_o_area\"]).float(),\n",
    "            lipid_droplet = torch.tensor(self.adata.obs[\"lipid_droplet_area\"]).float(),\n",
    "\n",
    "            # Distance to nearest amyloid\n",
    "            distance_to_nearest_amyloid = torch.tensor(self.adata.obs[\"lipid_droplet_area\"]).float()\n",
    "        )\n",
    "\n",
    "        return data\n",
    "\n",
    "    def create_edge_index(self):\n",
    "        tree = cKDTree(\n",
    "            self.adata.obs[['x_centroid', 'y_centroid']].values\n",
    "        )\n",
    "\n",
    "        _, neighbors = tree.query(\n",
    "            self.adata.obs[['x_centroid', 'y_centroid']].values, \n",
    "            k=31\n",
    "        )\n",
    "\n",
    "        rows = np.repeat(\n",
    "            np.arange(\n",
    "                len(adata.obs[['x_centroid', 'y_centroid']].values)\n",
    "            ), \n",
    "            30\n",
    "        )\n",
    "        cols = neighbors[:, 1:].reshape(-1)\n",
    "        return torch.tensor([rows, cols], dtype=torch.long)\n",
    "\n",
    "data = GraphDatasetGenerator(adata).generate_graph_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c22bbe2e",
   "metadata": {},
   "source": [
    "# Constructing our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "918bd0ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "5bc3410b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingEncoder(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(EmbeddingEncoder, self).__init__()\n",
    "\n",
    "        # Expression encoder\n",
    "        self.expression_encoder = Encoder(input_dim)\n",
    "\n",
    "        # Pathology heads\n",
    "        def mlp():\n",
    "            return nn.Sequential(\n",
    "                nn.Linear(1, 64),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(64, 8)\n",
    "            )\n",
    "\n",
    "        self.pathology_head = nn.ModuleDict({\n",
    "            \"oil_red_o\": mlp(),\n",
    "            \"plin2\": mlp(),\n",
    "            \"lipid_droplet\": mlp(),\n",
    "            \"distance_to_amyloid\": mlp()\n",
    "        })\n",
    "\n",
    "        self.embedding_head = nn.Linear(\n",
    "            in_features=input_dim + 8 * 4,\n",
    "            out_features=input_dim + 8 * 4\n",
    "        )\n",
    "\n",
    "    def forward(self, data):\n",
    "        # Encode expression\n",
    "        expression = self.expression_encoder(data.x)\n",
    "\n",
    "        # Encode pathology\n",
    "        pathology = torch.cat([\n",
    "            self.pathology_head[\"oil_red_o\"](data.oro.unsqueeze(1)),\n",
    "            self.pathology_head[\"plin2\"](data.plin2.unsqueeze(1)),\n",
    "            self.pathology_head[\"lipid_droplet\"](data.lipid_droplet.unsqueeze(1)),\n",
    "            self.pathology_head[\"distance_to_amyloid\"](data.distance_to_nearest_amyloid.unsqueeze(1)),\n",
    "        ], dim=1)\n",
    "\n",
    "        # Concatenate all embeddings\n",
    "        full_embedding = torch.cat([expression, pathology], dim=1)\n",
    "        \n",
    "        return full_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "bb3595b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = EmbeddingEncoder(\n",
    "    input_dim=data.x.shape[1], \n",
    ")\n",
    "output = model(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "250def42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0613, -0.0887,  0.0274,  ..., -0.4865,  0.2525, -0.5796],\n",
       "        [-0.0609, -0.2936, -0.0724,  ..., -0.4865,  0.2525, -0.5796],\n",
       "        [ 0.0424, -0.0849,  0.0429,  ..., -7.1261,  2.8584,  0.4881],\n",
       "        ...,\n",
       "        [ 0.0263, -0.2360,  0.1897,  ..., -0.4865,  0.2525, -0.5796],\n",
       "        [-0.0336, -0.1834,  0.1013,  ..., -0.4865,  0.2525, -0.5796],\n",
       "        [ 0.1763,  0.0120,  0.0502,  ..., -0.4865,  0.2525, -0.5796]],\n",
       "       grad_fn=<CatBackward0>)"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e65b6139",
   "metadata": {},
   "source": [
    "# Transformer Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e4440ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenGTTransformerLayer(nn.Module):\n",
    "\n",
    "    def __init__(self, input_dim, num_heads, dropout=0.1):\n",
    "        super(TokenGTTransformerLayer, self).__init__()\n",
    "        self.self_attn = nn.MultiHeadAttention(\n",
    "            embed_dim = input_dim,\n",
    "            num_heads = num_heads,\n",
    "            batch_first = True\n",
    "        )\n",
    "        self.norm1 = nn.LayerNorm(dim)\n",
    "        self.norm2 = nn.LayerNorm(dim)\n",
    "\n",
    "        self.ff = nn.Sequential(\n",
    "            nn.Linear(dim, dim * 4),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(dim * 4, dim),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self, x, attn_mask=None):\n",
    "        # x: [B, T, D]\n",
    "        residual = x\n",
    "        x, _ = self.self_attn(x, x, x, attn_mask=attn_mask)  # self-attention over tokens\n",
    "        x = self.norm1(x + residual)\n",
    "\n",
    "        residual = x\n",
    "        x = self.ff(x)\n",
    "        x = self.norm2(x + residual)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a2270cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd31ffeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenGT(nn.Module):\n",
    "\n",
    "    def __init__(self, input_dim):\n",
    "        super(TokenGT, self).__init__()\n",
    "\n",
    "        self.embedding_encoder = EmbeddingEncoder(input_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8fb5a2d",
   "metadata": {},
   "source": [
    "# Getting to go"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bdbf3cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CrossEntropyLoss()"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.CrossEntropyLoss()(\n",
    "    data.distance_to_nearest_amyloid < 60,\n",
    "    data\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3a94b72",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▏         | 966/69552 [00:04<05:31, 206.60it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[35]\u001b[39m\u001b[32m, line 168\u001b[39m\n\u001b[32m    166\u001b[39m example = []\n\u001b[32m    167\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m center \u001b[38;5;129;01min\u001b[39;00m tqdm(torch.randperm(data.num_nodes)):\n\u001b[32m--> \u001b[39m\u001b[32m168\u001b[39m     sub = \u001b[43mget_subgraph\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcenter_nodes\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43mcenter\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    169\u001b[39m     out = model(sub.to(device))[sub.node_mapping[\u001b[32m0\u001b[39m]]\n\u001b[32m    170\u001b[39m     mtoh_out = mtoh(out)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[35]\u001b[39m\u001b[32m, line 148\u001b[39m, in \u001b[36mget_subgraph\u001b[39m\u001b[34m(data, center_nodes, num_hops)\u001b[39m\n\u001b[32m    141\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget_subgraph\u001b[39m(data, center_nodes, num_hops=\u001b[32m1\u001b[39m):\n\u001b[32m    142\u001b[39m     subset, edge_index, mapping, edge_mask = k_hop_subgraph(\n\u001b[32m    143\u001b[39m         node_idx=center_nodes,\n\u001b[32m    144\u001b[39m         num_hops=num_hops,\n\u001b[32m    145\u001b[39m         edge_index=data.edge_index,\n\u001b[32m    146\u001b[39m         relabel_nodes=\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    147\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m148\u001b[39m     sub_data = \u001b[43mdata\u001b[49m\u001b[43m.\u001b[49m\u001b[43msubgraph\u001b[49m\u001b[43m(\u001b[49m\u001b[43msubset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    149\u001b[39m     sub_data.edge_index = edge_index\n\u001b[32m    150\u001b[39m     sub_data.edge_mask = edge_mask\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/Personal/TokenGT/.venv/lib/python3.13/site-packages/torch_geometric/data/data.py:725\u001b[39m, in \u001b[36mData.subgraph\u001b[39m\u001b[34m(self, subset)\u001b[39m\n\u001b[32m    718\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"Returns the induced subgraph given by the node indices\u001b[39;00m\n\u001b[32m    719\u001b[39m \u001b[33;03m:obj:`subset`.\u001b[39;00m\n\u001b[32m    720\u001b[39m \n\u001b[32m    721\u001b[39m \u001b[33;03mArgs:\u001b[39;00m\n\u001b[32m    722\u001b[39m \u001b[33;03m    subset (LongTensor or BoolTensor): The nodes to keep.\u001b[39;00m\n\u001b[32m    723\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    724\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m'\u001b[39m\u001b[33medge_index\u001b[39m\u001b[33m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m725\u001b[39m     edge_index, _, edge_mask = \u001b[43msubgraph\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    726\u001b[39m \u001b[43m        \u001b[49m\u001b[43msubset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    727\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43medge_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    728\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrelabel_nodes\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    729\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnum_nodes\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mnum_nodes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    730\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreturn_edge_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    731\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    732\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    733\u001b[39m     edge_index = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/Personal/TokenGT/.venv/lib/python3.13/site-packages/torch_geometric/utils/_subgraph.py:149\u001b[39m, in \u001b[36msubgraph\u001b[39m\u001b[34m(subset, edge_index, edge_attr, relabel_nodes, num_nodes, return_edge_mask)\u001b[39m\n\u001b[32m    145\u001b[39m edge_attr = edge_attr[edge_mask] \u001b[38;5;28;01mif\u001b[39;00m edge_attr \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    147\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m relabel_nodes:\n\u001b[32m    148\u001b[39m     edge_index, _ = map_index(\n\u001b[32m--> \u001b[39m\u001b[32m149\u001b[39m         \u001b[43medge_index\u001b[49m\u001b[43m.\u001b[49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[43m-\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m,\n\u001b[32m    150\u001b[39m         subset,\n\u001b[32m    151\u001b[39m         max_index=num_nodes,\n\u001b[32m    152\u001b[39m         inclusive=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m    153\u001b[39m     )\n\u001b[32m    154\u001b[39m     edge_index = edge_index.view(\u001b[32m2\u001b[39m, -\u001b[32m1\u001b[39m)\n\u001b[32m    156\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m return_edge_mask:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
    "from torch_geometric.utils import to_dense_batch\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "class TokenGT(nn.Module):\n",
    "    def __init__(self, input_dim, num_batches, num_nodes, d_model=128, nhead=8, num_layers=4):\n",
    "        super(TokenGT, self).__init__()\n",
    "        self.d_model = d_model\n",
    "\n",
    "        # === Node Encoder ===\n",
    "        self.node_encoder = EmbeddingEncoder(input_dim, num_batches)\n",
    "\n",
    "        # === Edge Encoder ===\n",
    "        self.edge_encoder = nn.Sequential(\n",
    "            nn.Linear(2 * d_model, d_model),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(d_model, d_model)\n",
    "        )\n",
    "\n",
    "        # === Type Embedding: 0=node, 1=edge, 2=graph token ===\n",
    "        self.type_embedding = nn.Embedding(3, d_model)\n",
    "\n",
    "        # === Position (Node ID) Embedding ===\n",
    "        self.position_embedding = nn.Embedding(num_nodes, d_model)\n",
    "\n",
    "        # === [GRAPH] token ===\n",
    "        self.graph_token = nn.Parameter(torch.randn(1, 1, d_model))\n",
    "\n",
    "        # === Transformer Encoder ===\n",
    "        encoder_layer = TransformerEncoderLayer(d_model=d_model, nhead=nhead, batch_first=True)\n",
    "        self.transformer = TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "\n",
    "        # === Prediction Head ===\n",
    "        self.output_head = nn.Sequential(\n",
    "            nn.Linear(d_model, d_model),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(d_model, 1)  # Adjust output dimension as needed\n",
    "        )\n",
    "\n",
    "    def forward(self, data):\n",
    "        B = data.x.shape[0]\n",
    "        N = data.x.size(0)\n",
    "        E = data.edge_index.size(1)\n",
    "\n",
    "        # === Node Tokens ===\n",
    "        node_embed = self.node_encoder(data)  # (N, d_model)\n",
    "\n",
    "        # === Edge Tokens ===\n",
    "        src = data.edge_index[0]\n",
    "        dst = data.edge_index[1]\n",
    "        edge_embed = self.edge_encoder(torch.cat([node_embed[src], node_embed[dst]], dim=1))  # (E, d_model)\n",
    "\n",
    "        # === Type and Position Embedding ===\n",
    "        node_type = self.type_embedding(torch.zeros(N, dtype=torch.long, device=node_embed.device))  # type 0\n",
    "        edge_type = self.type_embedding(torch.ones(E, dtype=torch.long, device=node_embed.device))   # type 1\n",
    "        node_pos = self.position_embedding(data.node_id if hasattr(data, \"node_id\") else torch.arange(N, device=node_embed.device))\n",
    "\n",
    "        node_tokens = node_embed + node_type + node_pos\n",
    "        edge_tokens = edge_embed + edge_type\n",
    "\n",
    "        # === [GRAPH] Token ===\n",
    "        graph_token = self.graph_token.expand(B, -1, -1)  # (B, 1, d_model)\n",
    "        \n",
    "        # === Batch Assembly ===\n",
    "        tokens = torch.cat([node_tokens, edge_tokens], dim=0)  # (N + E, d_model)\n",
    "        batch_vec = torch.cat([data.batch, data.batch[src]])   # Match token order to batch size\n",
    "\n",
    "        # === Dense Batch for Transformer ===\n",
    "        token_batch, mask = to_dense_batch(tokens, batch_vec)  # (B, T, d_model)\n",
    "\n",
    "        # Prepend [GRAPH] token\n",
    "        token_batch = torch.cat([graph_token, token_batch], dim=1)  # (B, T+1, d_model)\n",
    "        mask = torch.cat([torch.ones((B, 1), dtype=torch.bool, device=mask.device), mask], dim=1)\n",
    "\n",
    "        # === Transformer ===\n",
    "        out = self.transformer(token_batch, src_key_padding_mask=~mask)  # (B, T+1, d_model)\n",
    "\n",
    "        # === Predict from [GRAPH] token ===\n",
    "        graph_repr = out[:, 0]  # (B, d_model)\n",
    "        return self.output_head(graph_repr)\n",
    "\n",
    "\n",
    "class MultiTaskOutputHeads(nn.Module):\n",
    "    def __init__(self, in_dim, num_pathologies, num_nb_outputs=1, per_output_dispersion=False):\n",
    "        super().__init__()\n",
    "\n",
    "        self.num_nb_outputs = num_nb_outputs\n",
    "\n",
    "        # --- Count prediction (Negative Binomial) ---\n",
    "        self.count_mu_head = nn.Linear(in_dim, num_nb_outputs)  # predicts log_mu: (B, num_nb_outputs)\n",
    "\n",
    "        if per_output_dispersion:\n",
    "            self.log_dispersion = nn.Parameter(torch.zeros(num_nb_outputs))  # separate for each output\n",
    "        else:\n",
    "            self.log_dispersion = nn.Parameter(torch.tensor(0.0))  # shared\n",
    "\n",
    "        # --- Hurdle Log-Normal for pathologies ---\n",
    "        self.hurdle_zero_logits = nn.Linear(in_dim, num_pathologies)\n",
    "        self.hurdle_log_mu = nn.Linear(in_dim, num_pathologies)\n",
    "        self.hurdle_log_sigma = nn.Linear(in_dim, num_pathologies)\n",
    "\n",
    "        # --- Binary classification: proximity to amyloid ---\n",
    "        self.amyloid_head = nn.Linear(in_dim, 1)\n",
    "\n",
    "    def forward(self, center_token):\n",
    "        # --- NB count ---\n",
    "        log_mu = self.count_mu_head(center_token)  # (B, num_nb_outputs)\n",
    "        if self.log_dispersion.ndim == 0:\n",
    "            log_disp = self.log_dispersion.expand_as(log_mu)  # scalar -> (B, K)\n",
    "        else:\n",
    "            log_disp = self.log_dispersion.unsqueeze(0).expand(center_token.size(0), -1)  # (B, K)\n",
    "\n",
    "        # --- Hurdle Log-Normal ---\n",
    "        zero_logits = self.hurdle_zero_logits(center_token)\n",
    "        hln_log_mu = self.hurdle_log_mu(center_token)\n",
    "        hln_log_sigma = self.hurdle_log_sigma(center_token)\n",
    "\n",
    "        # --- Binary prediction ---\n",
    "        amyloid_logit = self.amyloid_head(center_token)\n",
    "\n",
    "        return {\n",
    "            \"nb\": {\n",
    "                \"log_mu\": log_mu,\n",
    "                \"log_dispersion\": log_disp\n",
    "            },\n",
    "            \"hurdle_lognorm\": {\n",
    "                \"zero_logits\": zero_logits,\n",
    "                \"log_mu\": hln_log_mu,\n",
    "                \"log_sigma\": hln_log_sigma\n",
    "            },\n",
    "            \"amyloid\": {\n",
    "                \"logits\": amyloid_logit\n",
    "            }\n",
    "        }\n",
    "\n",
    "from torch_geometric.utils import k_hop_subgraph\n",
    "\n",
    "def get_subgraph(data, center_nodes, num_hops=1):\n",
    "    subset, edge_index, mapping, edge_mask = k_hop_subgraph(\n",
    "        node_idx=center_nodes,\n",
    "        num_hops=num_hops,\n",
    "        edge_index=data.edge_index,\n",
    "        relabel_nodes=True\n",
    "    )\n",
    "    sub_data = data.subgraph(subset)\n",
    "    sub_data.edge_index = edge_index\n",
    "    sub_data.edge_mask = edge_mask\n",
    "    sub_data.node_mapping = mapping\n",
    "    return sub_data\n",
    "\n",
    "token_gt = TokenGT(data.x.shape[1], \n",
    "        adata.obs[\"folder\"].nunique(), \n",
    "        data.x.shape[0],\n",
    "        d_model=96, \n",
    "        nhead=8, \n",
    "        num_layers=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "df73ce28",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 4/69552 [00:00<22:30, 51.48it/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Expected parameter scale (Tensor of shape (3,)) of distribution Normal(loc: torch.Size([3]), scale: torch.Size([3])) to satisfy the constraint GreaterThan(lower_bound=0.0), but found invalid values:\ntensor([2.8416e-06, 0.0000e+00, 2.3265e-20], grad_fn=<ExpBackward0>)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[95]\u001b[39m\u001b[32m, line 16\u001b[39m\n\u001b[32m     10\u001b[39m mtoh_out = mtoh(out)\n\u001b[32m     11\u001b[39m nb_likelihood = NegativeBinomial(\n\u001b[32m     12\u001b[39m     logits = mtoh_out[\u001b[33m'\u001b[39m\u001b[33mnb\u001b[39m\u001b[33m'\u001b[39m][\u001b[33m'\u001b[39m\u001b[33mlog_mu\u001b[39m\u001b[33m'\u001b[39m],\n\u001b[32m     13\u001b[39m     total_count = mtoh_out[\u001b[33m'\u001b[39m\u001b[33mnb\u001b[39m\u001b[33m'\u001b[39m][\u001b[33m'\u001b[39m\u001b[33mlog_dispersion\u001b[39m\u001b[33m'\u001b[39m].exp()\n\u001b[32m     14\u001b[39m ).log_prob(data.counts[center]).sum()\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m pathology_likelihood = \u001b[43mhurdle_normal_log_prob\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     17\u001b[39m \u001b[43m    \u001b[49m\u001b[43mzero_logits\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mmtoh_out\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mhurdle_lognorm\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mzero_logits\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     18\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlog_mu\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mmtoh_out\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mhurdle_lognorm\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlog_mu\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     19\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlog_sigma\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mmtoh_out\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mhurdle_lognorm\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlog_sigma\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     20\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\n\u001b[32m     21\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlipid_droplet\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     22\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m.\u001b[49m\u001b[43mplin2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     23\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m.\u001b[49m\u001b[43moro\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     24\u001b[39m \u001b[43m    \u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mT\u001b[49m\u001b[43m[\u001b[49m\u001b[43mcenter\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m     25\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     27\u001b[39m loss = nb_likelihood + pathology_likelihood\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[56]\u001b[39m\u001b[32m, line 27\u001b[39m, in \u001b[36mhurdle_normal_log_prob\u001b[39m\u001b[34m(zero_logits, log_mu, log_sigma, data)\u001b[39m\n\u001b[32m     24\u001b[39m log_prob_zero = F.logsigmoid(-zero_logits)\n\u001b[32m     26\u001b[39m \u001b[38;5;66;03m# P(data > 0) = sigmoid(zero_logits) * Normal.log_prob(data)\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m27\u001b[39m normal = \u001b[43mNormal\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloc\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmu\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscale\u001b[49m\u001b[43m=\u001b[49m\u001b[43msigma\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     28\u001b[39m log_prob_nonzero = F.logsigmoid(zero_logits) + normal.log_prob(data)\n\u001b[32m     30\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m torch.where(is_zero, log_prob_zero, log_prob_nonzero)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/Personal/TokenGT/.venv/lib/python3.13/site-packages/torch/distributions/normal.py:60\u001b[39m, in \u001b[36mNormal.__init__\u001b[39m\u001b[34m(self, loc, scale, validate_args)\u001b[39m\n\u001b[32m     58\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     59\u001b[39m     batch_shape = \u001b[38;5;28mself\u001b[39m.loc.size()\n\u001b[32m---> \u001b[39m\u001b[32m60\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mbatch_shape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidate_args\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvalidate_args\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/Personal/TokenGT/.venv/lib/python3.13/site-packages/torch/distributions/distribution.py:72\u001b[39m, in \u001b[36mDistribution.__init__\u001b[39m\u001b[34m(self, batch_shape, event_shape, validate_args)\u001b[39m\n\u001b[32m     70\u001b[39m         valid = constraint.check(value)\n\u001b[32m     71\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch._is_all_true(valid):\n\u001b[32m---> \u001b[39m\u001b[32m72\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m     73\u001b[39m                 \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mExpected parameter \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparam\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     74\u001b[39m                 \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m(\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(value).\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m of shape \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtuple\u001b[39m(value.shape)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m) \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     75\u001b[39m                 \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mof distribution \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mrepr\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     76\u001b[39m                 \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mto satisfy the constraint \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mrepr\u001b[39m(constraint)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     77\u001b[39m                 \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mbut found invalid values:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mvalue\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m     78\u001b[39m             )\n\u001b[32m     79\u001b[39m \u001b[38;5;28msuper\u001b[39m().\u001b[34m__init__\u001b[39m()\n",
      "\u001b[31mValueError\u001b[39m: Expected parameter scale (Tensor of shape (3,)) of distribution Normal(loc: torch.Size([3]), scale: torch.Size([3])) to satisfy the constraint GreaterThan(lower_bound=0.0), but found invalid values:\ntensor([2.8416e-06, 0.0000e+00, 2.3265e-20], grad_fn=<ExpBackward0>)"
     ]
    }
   ],
   "source": [
    "\n",
    "mtoh = MultiTaskOutputHeads(96, 3, data.x.shape[1])\n",
    "\n",
    "batch_size = 1\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "example = []\n",
    "for center in tqdm(torch.randperm(data.num_nodes)):\n",
    "    sub = get_subgraph(data, center_nodes=[center])\n",
    "    out = model(sub.to(device))[sub.node_mapping[0]]\n",
    "    mtoh_out = mtoh(out)\n",
    "    nb_likelihood = NegativeBinomial(\n",
    "        logits = mtoh_out['nb']['log_mu'],\n",
    "        total_count = mtoh_out['nb']['log_dispersion'].exp()\n",
    "    ).log_prob(data.counts[center]).sum()\n",
    "\n",
    "    pathology_likelihood = hurdle_normal_log_prob(\n",
    "        zero_logits = mtoh_out[\"hurdle_lognorm\"][\"zero_logits\"],\n",
    "        log_mu = mtoh_out[\"hurdle_lognorm\"][\"log_mu\"],\n",
    "        log_sigma = mtoh_out[\"hurdle_lognorm\"][\"log_sigma\"],\n",
    "        data = torch.stack([\n",
    "            data.lipid_droplet,\n",
    "            data.plin2,\n",
    "            data.oro,\n",
    "        ]).T[center]\n",
    "    )\n",
    "\n",
    "    loss = nb_likelihood + pathology_likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fffe858f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:   0%|          | 1/69552 [00:00<7:14:18,  2.67it/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Expected parameter logits (Tensor of shape (366,)) of distribution NegativeBinomial(total_count: torch.Size([366]), logits: torch.Size([366])) to satisfy the constraint Real(), but found invalid values:\ntensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n        nan, nan, nan, nan, nan, nan], device='cuda:0',\n       grad_fn=<ViewBackward0>)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[96]\u001b[39m\u001b[32m, line 43\u001b[39m\n\u001b[32m     40\u001b[39m mtoh_out = mtoh(out)\n\u001b[32m     42\u001b[39m \u001b[38;5;66;03m# 3. Count loss (Negative Binomial)\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m43\u001b[39m nb_likelihood = \u001b[43mNegativeBinomial\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     44\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlogits\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmtoh_out\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mnb\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mlog_mu\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     45\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtotal_count\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmtoh_out\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mnb\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mlog_dispersion\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexp\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     46\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m.log_prob(data.counts[center].to(device)).sum()\n\u001b[32m     48\u001b[39m \u001b[38;5;66;03m# 4. Pathology loss (Hurdle Normal)\u001b[39;00m\n\u001b[32m     49\u001b[39m pathology_data = torch.stack([\n\u001b[32m     50\u001b[39m     data.lipid_droplet,\n\u001b[32m     51\u001b[39m     data.plin2,\n\u001b[32m     52\u001b[39m     data.oro,\n\u001b[32m     53\u001b[39m ], dim=\u001b[32m1\u001b[39m)[center].to(device)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/Personal/TokenGT/.venv/lib/python3.13/site-packages/torch/distributions/negative_binomial.py:61\u001b[39m, in \u001b[36mNegativeBinomial.__init__\u001b[39m\u001b[34m(self, total_count, probs, logits, validate_args)\u001b[39m\n\u001b[32m     59\u001b[39m \u001b[38;5;28mself\u001b[39m._param = \u001b[38;5;28mself\u001b[39m.probs \u001b[38;5;28;01mif\u001b[39;00m probs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m.logits\n\u001b[32m     60\u001b[39m batch_shape = \u001b[38;5;28mself\u001b[39m._param.size()\n\u001b[32m---> \u001b[39m\u001b[32m61\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mbatch_shape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidate_args\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvalidate_args\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/Personal/TokenGT/.venv/lib/python3.13/site-packages/torch/distributions/distribution.py:72\u001b[39m, in \u001b[36mDistribution.__init__\u001b[39m\u001b[34m(self, batch_shape, event_shape, validate_args)\u001b[39m\n\u001b[32m     70\u001b[39m         valid = constraint.check(value)\n\u001b[32m     71\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch._is_all_true(valid):\n\u001b[32m---> \u001b[39m\u001b[32m72\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m     73\u001b[39m                 \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mExpected parameter \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparam\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     74\u001b[39m                 \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m(\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(value).\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m of shape \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtuple\u001b[39m(value.shape)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m) \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     75\u001b[39m                 \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mof distribution \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mrepr\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     76\u001b[39m                 \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mto satisfy the constraint \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mrepr\u001b[39m(constraint)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     77\u001b[39m                 \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mbut found invalid values:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mvalue\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m     78\u001b[39m             )\n\u001b[32m     79\u001b[39m \u001b[38;5;28msuper\u001b[39m().\u001b[34m__init__\u001b[39m()\n",
      "\u001b[31mValueError\u001b[39m: Expected parameter logits (Tensor of shape (366,)) of distribution NegativeBinomial(total_count: torch.Size([366]), logits: torch.Size([366])) to satisfy the constraint Real(), but found invalid values:\ntensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n        nan, nan, nan, nan, nan, nan], device='cuda:0',\n       grad_fn=<ViewBackward0>)"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "# --- Assume you already have these ---\n",
    "# model: your main GNN or encoder\n",
    "# mtoh: MultiTaskOutputHeads(96, 3, data.x.shape[1])\n",
    "# data: graph with .counts, .lipid_droplet, etc.\n",
    "# get_subgraph: function for extracting neighborhood\n",
    "# hurdle_normal_log_prob: your custom function\n",
    "# NegativeBinomial: a distribution class (e.g., from torch.distributions or custom)\n",
    "\n",
    "# --- Training setup ---\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "mtoh = mtoh.to(device)\n",
    "\n",
    "optimizer = Adam(list(model.parameters()) + list(mtoh.parameters()), lr=1e-3)\n",
    "num_epochs = 10\n",
    "\n",
    "# --- Training loop ---\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    mtoh.train()\n",
    "\n",
    "    total_loss = 0.0\n",
    "    num_nodes = data.num_nodes\n",
    "\n",
    "    for center in tqdm(torch.randperm(num_nodes), desc=f\"Epoch {epoch+1}\"):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # 1. Subgraph extraction\n",
    "        sub = get_subgraph(data, center_nodes=[center])\n",
    "        sub = sub.to(device)\n",
    "\n",
    "        # 2. Forward pass through model\n",
    "        out = model(sub)[sub.node_mapping[0]]  # center node embedding\n",
    "        mtoh_out = mtoh(out)\n",
    "\n",
    "        # 3. Count loss (Negative Binomial)\n",
    "        nb_likelihood = NegativeBinomial(\n",
    "            logits=mtoh_out['nb']['log_mu'],\n",
    "            total_count=mtoh_out['nb']['log_dispersion'].exp()\n",
    "        ).log_prob(data.counts[center].to(device)).sum()\n",
    "\n",
    "        # 4. Pathology loss (Hurdle Normal)\n",
    "        pathology_data = torch.stack([\n",
    "            data.lipid_droplet,\n",
    "            data.plin2,\n",
    "            data.oro,\n",
    "        ], dim=1)[center].to(device)\n",
    "\n",
    "        pathology_likelihood = hurdle_normal_log_prob(\n",
    "            zero_logits=mtoh_out[\"hurdle_lognorm\"][\"zero_logits\"],\n",
    "            log_mu=mtoh_out[\"hurdle_lognorm\"][\"log_mu\"],\n",
    "            log_sigma=mtoh_out[\"hurdle_lognorm\"][\"log_sigma\"],\n",
    "            data=pathology_data\n",
    "        ).sum()\n",
    "\n",
    "        # 5. Total negative log-likelihood (we *minimize* -logprob)\n",
    "        loss = -(nb_likelihood + pathology_likelihood)\n",
    "\n",
    "        # 6. Backward + optimizer step\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    avg_loss = total_loss / num_nodes\n",
    "    print(f\"Epoch {epoch+1}, Avg Loss: {avg_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "b7972f2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Normal\n",
    "from torch.distributions import NegativeBinomial\n",
    "\n",
    "def hurdle_normal_log_prob(zero_logits, log_mu, log_sigma, data):\n",
    "    \"\"\"\n",
    "    Compute log-likelihood under a Hurdle Normal model.\n",
    "\n",
    "    Args:\n",
    "        zero_logits (Tensor): Logits for P(data > 0)\n",
    "        log_mu (Tensor): Log mean of the normal distribution\n",
    "        log_sigma (Tensor): Log std dev of the normal distribution\n",
    "        data (Tensor): Observed values (same shape as other inputs)\n",
    "\n",
    "    Returns:\n",
    "        Tensor: log-likelihoods (same shape as input tensors)\n",
    "    \"\"\"\n",
    "    sigma = log_sigma.exp()\n",
    "    mu = log_mu.exp()\n",
    "    is_zero = (data == 0)\n",
    "    \n",
    "    # P(data = 0) = sigmoid(-zero_logits)\n",
    "    log_prob_zero = F.logsigmoid(-zero_logits)\n",
    "\n",
    "    # P(data > 0) = sigmoid(zero_logits) * Normal.log_prob(data)\n",
    "    normal = Normal(loc=mu, scale=sigma)\n",
    "    log_prob_nonzero = F.logsigmoid(zero_logits) + normal.log_prob(data)\n",
    "\n",
    "    return torch.where(is_zero, log_prob_zero, log_prob_nonzero)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97818524",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
