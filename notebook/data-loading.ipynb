{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f448b95c",
   "metadata": {},
   "source": [
    "# Spatial Transcriptomics Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6949d849",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jhaberbe/Projects/Personal/TokenGT/.venv/lib/python3.13/site-packages/anndata/_core/anndata.py:1756: UserWarning: Observation names are not unique. To make them unique, call `.obs_names_make_unique`.\n",
      "  utils.warn_names_duplicates(\"obs\")\n",
      "/home/jhaberbe/Projects/Personal/TokenGT/.venv/lib/python3.13/site-packages/anndata/_core/storage.py:39: ImplicitModificationWarning: Layer 'counts' should not be a np.matrix, use np.ndarray instead.\n",
      "  warnings.warn(msg, ImplicitModificationWarning)\n"
     ]
    }
   ],
   "source": [
    "import scanpy as sc\n",
    "\n",
    "adata = sc.read_h5ad(\"../data/output-dgi-10-10-20MAY2025.h5ad\")\n",
    "adata.layers[\"counts\"] = adata.layers[\"counts\"].todense()\n",
    "adata = adata[adata.obs[\"folder\"].eq(\"05-27\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d76cb318",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b2aafc2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm \n",
    "\n",
    "import torch\n",
    "from torch_geometric.data import Data, InMemoryDataset\n",
    "from torch_geometric.utils import k_hop_subgraph\n",
    "import numpy as np\n",
    "from scipy.spatial import cKDTree\n",
    "\n",
    "class GraphDatasetGenerator:\n",
    "    def __init__(self, adata, k=1):\n",
    "        self.adata = adata\n",
    "        self.k = k\n",
    "        self.edge_index = self.create_edge_index()\n",
    "        self.node_features = torch.tensor(adata.X).float()\n",
    "        self.counts = torch.tensor(adata.layers[\"counts\"]).float()\n",
    "        self.batch = torch.tensor(adata.obs[\"folder\"].cat.codes).long()\n",
    "        self.metadata = {\n",
    "            \"plin2\": torch.tensor(adata.obs[\"plin2_area\"]).float(),\n",
    "            \"oro\": torch.tensor(adata.obs[\"oil_red_o_area\"]).float(),\n",
    "            \"lipid_droplet\": torch.tensor(adata.obs[\"lipid_droplet_area\"]).float(),\n",
    "            \"distance_to_nearest_amyloid\": torch.tensor(adata.obs[\"distance_to_nearest_amyloid\"]).float(),\n",
    "        }\n",
    "\n",
    "    def create_edge_index(self):\n",
    "        coords = self.adata.obs[['x_centroid', 'y_centroid']].values\n",
    "        tree = cKDTree(coords)\n",
    "        _, neighbors = tree.query(coords, k=31)\n",
    "        rows = np.repeat(np.arange(len(coords)), 30)\n",
    "        cols = neighbors[:, 1:].reshape(-1)\n",
    "        return torch.tensor([rows, cols], dtype=torch.long)\n",
    "\n",
    "    def generate_subgraphs(self):\n",
    "        subgraphs = []\n",
    "        num_nodes = self.node_features.size(0)\n",
    "\n",
    "        for center_node in tqdm(range(num_nodes)):\n",
    "            subset, edge_index, mapping, _ = k_hop_subgraph(\n",
    "                center_node, self.k, self.edge_index, relabel_nodes=True\n",
    "            )\n",
    "\n",
    "            # Build subgraph data\n",
    "            data = Data(\n",
    "                x=self.node_features[subset],\n",
    "                counts=self.counts[subset],\n",
    "                batch=self.batch[subset],\n",
    "                edge_index=edge_index,\n",
    "                center_idx=mapping,  # index of center node in subgraph\n",
    "            )\n",
    "\n",
    "            for key, tensor in self.metadata.items():\n",
    "                data[key] = tensor[subset]\n",
    "\n",
    "            subgraphs.append(data)\n",
    "\n",
    "        return subgraphs\n",
    "\n",
    "class SubgraphDataset(InMemoryDataset):\n",
    "    def __init__(self, adata, transform=None):\n",
    "        self.generator = GraphDatasetGenerator(adata)\n",
    "        self.subgraphs = self.generator.generate_subgraphs()\n",
    "        super().__init__(None, transform)\n",
    "\n",
    "    def len(self):\n",
    "        return len(self.subgraphs)\n",
    "\n",
    "    def get(self, idx):\n",
    "        return self.subgraphs[idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fc5a84b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3340137/1968146773.py:30: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:254.)\n",
      "  return torch.tensor([rows, cols], dtype=torch.long)\n",
      "/tmp/ipykernel_3340137/1968146773.py:16: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  self.batch = torch.tensor(adata.obs[\"folder\"].cat.codes).long()\n",
      "/tmp/ipykernel_3340137/1968146773.py:18: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  \"plin2\": torch.tensor(adata.obs[\"plin2_area\"]).float(),\n",
      "/tmp/ipykernel_3340137/1968146773.py:19: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  \"oro\": torch.tensor(adata.obs[\"oil_red_o_area\"]).float(),\n",
      "/tmp/ipykernel_3340137/1968146773.py:20: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  \"lipid_droplet\": torch.tensor(adata.obs[\"lipid_droplet_area\"]).float(),\n",
      "/tmp/ipykernel_3340137/1968146773.py:21: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  \"distance_to_nearest_amyloid\": torch.tensor(adata.obs[\"distance_to_nearest_amyloid\"]).float(),\n",
      "100%|██████████| 69552/69552 [02:50<00:00, 407.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data(x=[11, 366], edge_index=[2, 70], counts=[11, 366], batch=[11], center_idx=[1], plin2=[11], oro=[11], lipid_droplet=[11], distance_to_nearest_amyloid=[11])\n",
      "Center node index: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "dataset = SubgraphDataset(adata)\n",
    "\n",
    "# Example\n",
    "sample = dataset[0]\n",
    "print(sample)\n",
    "print(\"Center node index:\", sample.center_idx.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "32a56dd6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data(x=[11, 366], edge_index=[2, 70], counts=[11, 366], batch=[11], center_idx=[1], plin2=[11], oro=[11], lipid_droplet=[11], distance_to_nearest_amyloid=[11])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a60ce9a1",
   "metadata": {},
   "source": [
    "# Encoding Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1c40188c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# === Expression + Pathology Node Encoder ===\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc2(self.relu(self.fc1(x)))\n",
    "\n",
    "class NodeEncodingLayer(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super().__init__()\n",
    "        self.expression_encoder = Encoder(input_dim)\n",
    "\n",
    "        def mlp():\n",
    "            return nn.Sequential(\n",
    "                nn.Linear(1, 64),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(64, 8)\n",
    "            )\n",
    "\n",
    "        self.pathology_head = nn.ModuleDict({\n",
    "            \"oil_red_o\": mlp(),\n",
    "            \"plin2\": mlp(),\n",
    "            \"lipid_droplet\": mlp(),\n",
    "            \"distance_to_amyloid\": mlp()\n",
    "        })\n",
    "\n",
    "        self.project = nn.Linear(8*4 + 64, output_dim)\n",
    "\n",
    "    def forward(self, data):\n",
    "        expression = self.expression_encoder(data.x)\n",
    "        pathology = torch.cat([\n",
    "            self.pathology_head[\"oil_red_o\"](data.oro.unsqueeze(1)),\n",
    "            self.pathology_head[\"plin2\"](data.plin2.unsqueeze(1)),\n",
    "            self.pathology_head[\"lipid_droplet\"](data.lipid_droplet.unsqueeze(1)),\n",
    "            self.pathology_head[\"distance_to_amyloid\"](data.distance_to_nearest_amyloid.unsqueeze(1)),\n",
    "        ], dim=1)\n",
    "        return self.project(torch.cat([expression, pathology], dim=1))  # [N, D]\n",
    "\n",
    "# === Edge Encoder ===\n",
    "class EdgeEncodingLayer(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super().__init__()\n",
    "        self.edge_mlp = nn.Sequential(\n",
    "            nn.Linear(2 * input_dim, input_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(input_dim, input_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, node_embeddings, edge_index):\n",
    "        src = node_embeddings[edge_index[0]]\n",
    "        tgt = node_embeddings[edge_index[1]]\n",
    "        edge_input = torch.cat([src, tgt], dim=1)\n",
    "        return self.edge_mlp(edge_input)\n",
    "\n",
    "# === Type Embedding Layer ===\n",
    "class TokenTypeEmbedding(nn.Module):\n",
    "    def __init__(self, num_types: int, dim: int):\n",
    "        super().__init__()\n",
    "        self.type_embedding = nn.Embedding(num_types, dim)\n",
    "\n",
    "    def forward(self, token_embeddings: torch.Tensor, type_ids: torch.Tensor):\n",
    "        return token_embeddings + self.type_embedding(type_ids)\n",
    "\n",
    "# === Full Graph Tokenizer ===\n",
    "class GraphTokenizer(nn.Module):\n",
    "    def __init__(self, input_dim: int, hidden_dim: int):\n",
    "        super().__init__()\n",
    "        self.node_encoder = NodeEncodingLayer(input_dim, hidden_dim)\n",
    "        self.edge_encoder = EdgeEncodingLayer(hidden_dim)\n",
    "        self.token_type_embedder = TokenTypeEmbedding(num_types=3, dim=hidden_dim)\n",
    "        self.graph_token = nn.Parameter(torch.randn(1, hidden_dim))  # learnable [CLS] token\n",
    "\n",
    "    def forward(self, data):\n",
    "        # Node tokens\n",
    "        device = data.x.device\n",
    "        node_tokens = self.node_encoder(data)  # [N, D]\n",
    "        node_type_ids = torch.zeros(node_tokens.size(0), dtype=torch.long, device=device)\n",
    "\n",
    "        # Edge tokens\n",
    "        edge_tokens = self.edge_encoder(node_tokens, data.edge_index)  # [E, D]\n",
    "        edge_type_ids = torch.ones(edge_tokens.size(0), dtype=torch.long, device=device)\n",
    "\n",
    "        # Graph token\n",
    "        graph_token = self.graph_token.expand(1, -1)  # [1, D]\n",
    "        graph_type_id = torch.tensor([2], dtype=torch.long, device=node_tokens.device)\n",
    "\n",
    "        # Combine tokens and types\n",
    "        tokens = torch.cat([node_tokens, edge_tokens, graph_token], dim=0)  # [T, D]\n",
    "        type_ids = torch.cat([node_type_ids, edge_type_ids, graph_type_id], dim=0)  # [T]\n",
    "\n",
    "        # Add type embeddings\n",
    "        tokens = self.token_type_embedder(tokens.unsqueeze(0), type_ids.unsqueeze(0))  # [1, T, D]\n",
    "\n",
    "        # Index of center node token (in concatenated tokens, node tokens first)\n",
    "        center_token_idx = int(data.center_idx)\n",
    "\n",
    "        return tokens.squeeze(0), type_ids, center_token_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "189e7a5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([828, 256])\n",
      "torch.Size([828])\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "tokenizer = GraphTokenizer(input_dim=sample.x.shape[1], hidden_dim=256)\n",
    "tokens, type_ids, center_token_idx = tokenizer(dataset[4])\n",
    "\n",
    "print(tokens.shape)    # [T, 256]\n",
    "print(type_ids.shape)  # [T]\n",
    "print(center_token_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad3aa7cc",
   "metadata": {},
   "source": [
    "# Transformer Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "68450aac",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerStack(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, num_layers, dropout=0.1):\n",
    "        super().__init__()\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model,\n",
    "            nhead=num_heads,\n",
    "            dim_feedforward=d_model * 4,\n",
    "            dropout=dropout,\n",
    "            activation=\"gelu\"\n",
    "        )\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "\n",
    "    def forward(self, x, src_key_padding_mask=None):\n",
    "        # x: [T, B, D] for nn.Transformer modules (T=seq len, B=batch, D=features)\n",
    "        # but we have [B, T, D] or [T, D], so be careful to transpose if needed\n",
    "        if x.dim() == 3:\n",
    "            x = x.transpose(0, 1)  # to [T, B, D]\n",
    "\n",
    "        out = self.transformer(x, src_key_padding_mask=src_key_padding_mask)\n",
    "\n",
    "        return out.transpose(0, 1)  # back to [B, T, D] or squeeze batch if needed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "40e47b63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Center node embedding: torch.Size([256])\n",
      "Graph CLS embedding: torch.Size([256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jhaberbe/Projects/Personal/TokenGT/.venv/lib/python3.13/site-packages/torch/nn/modules/transformer.py:382: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "tokens, type_ids, center_idx = tokenizer(sample)\n",
    "tokens = tokens.unsqueeze(1)  # [T, 1, D]\n",
    "\n",
    "transformer_stack = TransformerStack(d_model=tokens.size(-1), num_heads=8, num_layers=6)\n",
    "encoded = transformer_stack(tokens)  # [T, 1, D]\n",
    "\n",
    "encoded = encoded.squeeze(1)  # [T, D]\n",
    "center_embedding = encoded[center_idx]\n",
    "cls_embedding = encoded[-1]  # the last token = CLS token\n",
    "\n",
    "print(\"Center node embedding:\", center_embedding.shape)\n",
    "print(\"Graph CLS embedding:\", cls_embedding.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed7b9eeb",
   "metadata": {},
   "source": [
    "# Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "657b1ce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import NegativeBinomial, Normal, Bernoulli\n",
    "\n",
    "class MultiTaskOutputHead(nn.Module):\n",
    "    def __init__(self, embedding_dim, num_counts=366):\n",
    "        super().__init__()\n",
    "        # Counts NB mean logits per feature\n",
    "        self.nb_mean_logits = nn.Linear(embedding_dim, num_counts)  # [D -> num_counts]\n",
    "\n",
    "        # Shared dispersion (scalar, learnable positive parameter via softplus)\n",
    "        self.log_dispersion = nn.Parameter(torch.tensor(0.0))\n",
    "\n",
    "        # Hurdle Normal heads for plin2, oro, lipid_droplet (log1p inputs)\n",
    "        # Each outputs: mixture logits (zero/nonzero), mean, and log std dev\n",
    "        def hurdle_net():\n",
    "            return nn.Sequential(\n",
    "                nn.Linear(embedding_dim, 64),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(64, 3)  # logits for zero/nonzero, mean, log_std\n",
    "            )\n",
    "\n",
    "        self.hurdle_plin2 = hurdle_net()\n",
    "        self.hurdle_oro = hurdle_net()\n",
    "        self.hurdle_lipid = hurdle_net()\n",
    "\n",
    "        # Amyloid soft-binarization parameters:\n",
    "        # alpha controls sharpness of sigmoid cutoff at 60\n",
    "        self.threshold = 60.0\n",
    "\n",
    "        # Amyloid prediction head outputs predicted distance\n",
    "        self.amyloid_head = nn.Sequential(\n",
    "            nn.Linear(embedding_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 1)  # predicted distance scalar\n",
    "        )\n",
    "\n",
    "    def forward(self, embedding):\n",
    "        \"\"\"\n",
    "        embedding: [batch_size, embedding_dim]\n",
    "        Returns dict of params for losses\n",
    "        \"\"\"\n",
    "\n",
    "        out = {}\n",
    "\n",
    "        # Counts NB mean logits\n",
    "        out[\"nb_logits\"] = self.nb_mean_logits(embedding)  # [B, num_counts]\n",
    "\n",
    "        # Dispersion param (positive)\n",
    "        out[\"dispersion\"] = F.softplus(self.log_dispersion) + 1e-6\n",
    "\n",
    "        # Hurdle normal params\n",
    "        def hurdle_params(net, emb):\n",
    "            params = net(emb)  # [B, 3]\n",
    "            logits = params[:, 0]         # zero vs nonzero logit\n",
    "            mean = params[:, 1]           # mean of normal\n",
    "            log_std = params[:, 2].clamp(-10, 2)  # stability clamp\n",
    "            std = torch.exp(log_std)\n",
    "            return logits, mean, std\n",
    "\n",
    "        out[\"plin2_logits\"], out[\"plin2_mean\"], out[\"plin2_std\"] = hurdle_params(self.hurdle_plin2, embedding)\n",
    "        out[\"oro_logits\"], out[\"oro_mean\"], out[\"oro_std\"] = hurdle_params(self.hurdle_oro, embedding)\n",
    "        out[\"lipid_logits\"], out[\"lipid_mean\"], out[\"lipid_std\"] = hurdle_params(self.hurdle_lipid, embedding)\n",
    "\n",
    "        # Amyloid distance prediction (continuous)\n",
    "        out[\"amyloid_dist_pred\"] = self.amyloid_head(embedding).squeeze(-1)  # [B]\n",
    "\n",
    "        return out\n",
    "\n",
    "mtoh = MultiTaskOutputHead(256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "86023668",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'nb_logits': tensor([[-2.0795e-02,  6.1179e-01, -8.8001e-02, -6.3799e-02,  7.9653e-03,\n",
       "           3.4802e-01, -9.2017e-01, -4.4168e-01,  3.1161e-01, -2.2652e-01,\n",
       "          -4.1290e-01,  3.5762e-01,  1.1216e-01,  8.2503e-01,  1.3417e+00,\n",
       "          -1.0144e+00, -4.6209e-01, -3.5184e-01, -6.7561e-01, -5.7450e-01,\n",
       "          -5.0079e-02,  2.9879e-01,  1.1364e+00,  3.2880e-01, -5.5709e-01,\n",
       "          -5.9692e-01,  2.3268e-01, -2.9988e-01,  1.1858e-01, -3.3631e-01,\n",
       "          -1.1073e+00,  2.3833e-03, -7.3384e-02,  8.4417e-01, -1.8533e-01,\n",
       "          -5.5096e-01,  6.4281e-01, -2.2141e-01, -7.5244e-02, -9.5823e-02,\n",
       "           3.9052e-01,  3.4569e-01,  2.7870e-01,  8.8941e-02, -6.2073e-01,\n",
       "          -4.1123e-01, -6.8186e-01, -6.1696e-01,  4.0754e-01, -2.6103e-01,\n",
       "           6.4406e-01, -1.0197e+00,  7.2537e-01,  1.6940e-01, -6.6164e-01,\n",
       "          -5.9702e-01,  5.1688e-01,  2.4048e-01,  1.0551e+00, -4.1741e-01,\n",
       "           3.5203e-01, -3.1355e-01,  3.2934e-01, -2.4028e-01,  3.1110e-01,\n",
       "          -3.7113e-01,  4.3517e-01, -2.8204e-01, -4.8359e-01,  2.0075e-01,\n",
       "           4.8852e-01,  2.8755e-01, -4.6199e-01,  1.2484e+00,  2.0068e-02,\n",
       "           6.5587e-01,  6.6389e-01, -1.5994e-02, -1.3836e-01, -8.5618e-02,\n",
       "           4.3826e-01, -7.6111e-01, -5.0305e-01,  1.1257e+00, -3.9113e-01,\n",
       "           1.9012e-01,  4.8442e-01,  1.4733e+00, -2.9343e-01,  2.9593e-01,\n",
       "           5.1277e-01,  6.7616e-01, -7.3144e-01,  1.1258e+00, -1.2884e-01,\n",
       "           1.3964e-01,  8.9265e-01, -1.0553e+00,  8.7805e-01,  1.1104e+00,\n",
       "          -7.2935e-01,  1.4021e+00,  1.5710e-01, -2.1006e-01, -1.2078e+00,\n",
       "           5.0612e-01,  1.0203e+00, -4.5595e-01, -6.3375e-01, -3.2754e-01,\n",
       "           4.5635e-01, -6.6682e-01, -1.1421e-01,  2.6848e-01,  1.5582e-01,\n",
       "          -1.9947e-01,  1.5801e-02, -7.6956e-01, -6.7641e-01, -5.4066e-01,\n",
       "           1.4672e+00,  4.0185e-01,  3.8026e-01,  8.4201e-01, -4.1635e-01,\n",
       "          -1.4735e-01,  5.8859e-01,  3.4934e-01,  4.5924e-01, -6.9247e-01,\n",
       "          -5.1446e-01,  4.8101e-05,  6.0894e-01, -7.4452e-02, -4.6068e-01,\n",
       "          -6.7578e-01,  4.3428e-01, -1.2558e-01, -6.2477e-01,  4.6601e-01,\n",
       "           1.9626e-01, -3.3992e-01, -5.3585e-01,  8.4130e-01,  3.5554e-01,\n",
       "          -6.3342e-01, -1.3349e+00, -1.9209e-01, -3.7378e-01, -3.9689e-01,\n",
       "           5.1185e-01, -7.1149e-01, -2.2247e-01, -1.1353e-01,  3.2084e-01,\n",
       "          -9.7212e-01,  3.1764e-02, -1.7509e-02, -3.4073e-01,  2.5129e-01,\n",
       "          -3.4155e-02, -5.7689e-01, -8.6753e-01,  4.0707e-01,  5.4024e-01,\n",
       "           1.8375e-01,  9.1014e-02, -8.5450e-01,  4.8532e-01, -3.3938e-01,\n",
       "          -1.7198e+00, -3.8038e-01, -7.5118e-01,  9.2039e-01,  1.3289e+00,\n",
       "          -6.4846e-02,  5.9043e-02,  1.8347e-01, -5.0243e-02, -1.2380e+00,\n",
       "           7.5258e-01, -3.1420e-01,  3.7443e-01, -1.0381e+00, -8.6547e-02,\n",
       "          -9.3774e-01, -1.6901e-01,  4.9323e-01,  2.6406e-01, -1.6982e-01,\n",
       "           4.0188e-01, -8.0175e-01,  5.9988e-02,  6.9614e-02,  2.4628e-01,\n",
       "          -7.7802e-01,  4.4306e-01,  3.2101e-01, -3.1869e-01,  7.6017e-01,\n",
       "           2.6190e-01,  9.4200e-01,  1.1421e+00,  3.3758e-01, -2.6715e-01,\n",
       "          -7.5422e-02, -7.4277e-02,  3.0239e-01,  1.0958e-01, -4.6641e-01,\n",
       "          -6.3634e-01, -9.8380e-02,  6.8728e-01, -5.2574e-02,  4.8823e-01,\n",
       "          -7.3584e-02, -4.6201e-01, -2.0271e+00,  5.6857e-01,  1.2764e-01,\n",
       "          -4.5914e-01,  3.0059e-01,  3.1550e-01, -1.2135e-01, -9.7178e-01,\n",
       "           5.6156e-01, -3.5192e-01,  2.3646e-01,  5.3803e-02, -7.9745e-01,\n",
       "           1.2843e-01, -8.5230e-01,  5.6334e-01, -5.0933e-01,  6.3370e-02,\n",
       "           1.0273e+00,  1.9270e+00,  8.2452e-01, -6.7519e-01, -1.1237e+00,\n",
       "          -2.8060e-01, -5.9568e-01, -6.0978e-01, -4.6503e-01,  8.9682e-01,\n",
       "           2.6447e-01, -9.1983e-01,  4.2614e-02, -3.4921e-02, -8.6341e-01,\n",
       "           6.5889e-01,  5.4934e-02, -3.4720e-01, -4.3546e-01, -9.1749e-02,\n",
       "          -8.2336e-01, -4.8631e-02, -4.8496e-01,  5.2841e-01, -8.9110e-01,\n",
       "          -1.0355e-01,  2.4744e-01,  9.3490e-01,  2.4475e-01, -1.0020e-01,\n",
       "           9.6548e-01,  2.8038e-01,  2.1975e-01,  4.6827e-01,  7.2761e-01,\n",
       "          -7.9001e-01, -2.4396e-01,  3.1086e-01,  7.1445e-01,  1.1504e+00,\n",
       "           6.3261e-01,  1.5078e-02,  6.8575e-01,  4.5992e-01, -4.6338e-02,\n",
       "          -8.1203e-03,  1.0973e+00, -1.4575e-01,  3.2606e-01, -2.7658e-01,\n",
       "           1.6994e-01,  4.4980e-01,  6.1784e-01, -1.3703e-01,  1.1373e+00,\n",
       "           5.0517e-01,  2.5516e-01,  1.1569e+00, -2.9585e-01, -2.4515e-02,\n",
       "           9.1986e-01,  7.4591e-01,  2.2146e-01, -3.7560e-01, -8.3419e-01,\n",
       "           7.5919e-01, -7.4974e-01, -4.2420e-02, -3.6228e-01,  4.5742e-02,\n",
       "           2.8063e-01,  2.2980e-02,  7.6371e-02, -1.5522e+00, -6.8123e-01,\n",
       "          -5.2743e-01, -3.3782e-01, -2.5580e-01, -6.0525e-01, -6.1576e-01,\n",
       "          -9.6046e-01, -5.1502e-01, -5.6231e-01, -2.4380e-02, -4.0466e-01,\n",
       "           9.0042e-01,  1.2412e-01,  1.2927e+00, -7.3533e-01, -1.1507e+00,\n",
       "           2.9961e-01,  3.0285e-01, -7.4211e-01, -6.6267e-01, -6.4617e-01,\n",
       "           1.3982e-01,  1.1869e-01, -1.8007e-01,  9.5002e-01,  7.9212e-01,\n",
       "          -4.5365e-01, -3.0749e-01,  1.5974e-01,  1.1154e-01,  3.3109e-02,\n",
       "          -7.6262e-01, -1.4440e+00, -1.0859e+00,  8.5270e-01, -9.0250e-01,\n",
       "          -6.5702e-01,  2.0383e-01,  5.9327e-01, -6.5366e-01, -4.5708e-01,\n",
       "          -3.4500e-01,  4.5427e-01,  3.4582e-02, -1.5538e+00,  2.3157e-01,\n",
       "           5.5780e-01,  2.1826e-01, -2.5292e-02, -1.5471e-01, -1.0716e+00,\n",
       "          -1.9738e-01, -3.7173e-01, -2.1193e-01, -1.1668e+00, -7.0021e-01,\n",
       "           9.3256e-01]], grad_fn=<AddmmBackward0>),\n",
       " 'dispersion': tensor(0.6931, grad_fn=<AddBackward0>),\n",
       " 'plin2_logits': tensor([0.0493], grad_fn=<SelectBackward0>),\n",
       " 'plin2_mean': tensor([0.3897], grad_fn=<SelectBackward0>),\n",
       " 'plin2_std': tensor([1.5417], grad_fn=<ExpBackward0>),\n",
       " 'oro_logits': tensor([-0.0975], grad_fn=<SelectBackward0>),\n",
       " 'oro_mean': tensor([-0.1310], grad_fn=<SelectBackward0>),\n",
       " 'oro_std': tensor([0.9540], grad_fn=<ExpBackward0>),\n",
       " 'lipid_logits': tensor([-0.2602], grad_fn=<SelectBackward0>),\n",
       " 'lipid_mean': tensor([-0.2174], grad_fn=<SelectBackward0>),\n",
       " 'lipid_std': tensor([1.2379], grad_fn=<ExpBackward0>),\n",
       " 'amyloid_dist_pred': tensor([0.1913], grad_fn=<SqueezeBackward1>)}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mtoh(center_embedding.unsqueeze(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6292f43e",
   "metadata": {},
   "source": [
    "# Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2edcfdee",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DGIDiscriminator(nn.Module):\n",
    "    def __init__(self, embed_dim):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(embed_dim, embed_dim)\n",
    "\n",
    "    def forward(self, global_emb, node_embs):\n",
    "        # global_emb: [D]\n",
    "        # node_embs: [N, D]\n",
    "        global_proj = self.linear(global_emb)   # [D]\n",
    "        scores = torch.matmul(node_embs, global_proj)  # [N]\n",
    "        return scores  # raw scores for BCEWithLogitsLoss\n",
    "    \n",
    "def dgi_loss(discriminator, cls_emb, node_embs, corrupted_cls_emb, corrupted_node_embs):\n",
    "    pos_scores = discriminator(cls_emb, node_embs)             # label=1\n",
    "    neg_scores = discriminator(corrupted_cls_emb, corrupted_node_embs)  # label=0\n",
    "    \n",
    "    labels_pos = torch.ones_like(pos_scores)\n",
    "    labels_neg = torch.zeros_like(neg_scores)\n",
    "    \n",
    "    loss_pos = nn.BCEWithLogitsLoss()(pos_scores, labels_pos)\n",
    "    loss_neg = nn.BCEWithLogitsLoss()(neg_scores, labels_neg)\n",
    "    \n",
    "    return loss_pos + loss_neg\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1243a57b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nb_loss(logits, dispersion, counts):\n",
    "    # logits: [B, F] (mean logits)\n",
    "    # dispersion: scalar > 0\n",
    "    # counts: [B, F] integer counts\n",
    "\n",
    "    nb = NegativeBinomial(total_count=dispersion, logits=logits)\n",
    "    return -nb.log_prob(counts).mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ed0c3d41",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hurdle_normal_loss(logits, mean, std, target):\n",
    "    # target: raw values (positive or zero)\n",
    "    # We model log1p(target)\n",
    "    x = torch.log1p(target)\n",
    "\n",
    "    p_zero = torch.sigmoid(-logits)  # probability of zero\n",
    "\n",
    "    zero_mask = (target == 0).float()\n",
    "    nonzero_mask = 1 - zero_mask\n",
    "\n",
    "    normal_dist = Normal(mean, std)\n",
    "\n",
    "    # Log prob for zero\n",
    "    log_prob_zero = torch.log(p_zero + 1e-8)\n",
    "\n",
    "    # Log prob for nonzero\n",
    "    log_prob_nonzero = torch.log(1 - p_zero + 1e-8) + normal_dist.log_prob(x)\n",
    "\n",
    "    loss = -(\n",
    "        zero_mask * log_prob_zero +\n",
    "        nonzero_mask * log_prob_nonzero\n",
    "    )\n",
    "\n",
    "    return loss.mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d5140429",
   "metadata": {},
   "outputs": [],
   "source": [
    "def amyloid_soft_binarization_loss(dist_pred, dist_true, alpha, threshold=60.0):\n",
    "    # dist_true, dist_pred: [B] floats\n",
    "    # alpha: scalar > 0\n",
    "\n",
    "    y = torch.sigmoid(-alpha * (dist_true - threshold))\n",
    "    y_pred = torch.sigmoid(-alpha * (dist_pred - threshold))\n",
    "\n",
    "    bce = F.binary_cross_entropy(y_pred, y)\n",
    "    return bce\n",
    "\n",
    "def amyloid_bce_loss(dist_pred, dist_true, threshold=60.0):\n",
    "    # Binarize the true distances at the threshold\n",
    "    y = (dist_true < threshold).float()  # 1 if dist < 60, else 0\n",
    "    \n",
    "    # Apply sigmoid to predicted logits\n",
    "    y_pred = torch.sigmoid(dist_pred)\n",
    "    \n",
    "    # Compute BCE loss\n",
    "    bce = nn.functional.binary_cross_entropy(y_pred, y)\n",
    "    return bce\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a54420f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def total_loss(outputs, batch):\n",
    "    \"\"\"\n",
    "    outputs: dict from MultiTaskOutputHead.forward()\n",
    "    batch: data object with ground truth fields:\n",
    "        counts, plin2, oro, lipid_droplet, distance_to_nearest_amyloid\n",
    "    \"\"\"\n",
    "\n",
    "    loss = 0.0\n",
    "\n",
    "    # NB counts loss\n",
    "    loss += nb_loss(outputs[\"nb_logits\"], outputs[\"dispersion\"], batch[\"counts\"])\n",
    "\n",
    "    # Hurdle normal losses (log1p inputs)\n",
    "    loss += hurdle_normal_loss(outputs[\"plin2_logits\"], outputs[\"plin2_mean\"], outputs[\"plin2_std\"], batch[\"plin2\"])\n",
    "    loss += hurdle_normal_loss(outputs[\"oro_logits\"], outputs[\"oro_mean\"], outputs[\"oro_std\"], batch[\"oro\"])\n",
    "    loss += hurdle_normal_loss(outputs[\"lipid_logits\"], outputs[\"lipid_mean\"], outputs[\"lipid_std\"], batch[\"lipid_droplet\"])\n",
    "\n",
    "    # Amyloid soft binarization loss\n",
    "    loss += amyloid_bce_loss(outputs[\"amyloid_dist_pred\"], batch[\"distance_to_nearest_amyloid\"])\n",
    "\n",
    "    return loss\n",
    "\n",
    "def total_loss(outputs, batch):\n",
    "    \"\"\"\n",
    "    outputs: dict from MultiTaskOutputHead.forward()\n",
    "    batch: data object with ground truth fields:\n",
    "        counts, plin2, oro, lipid_droplet, distance_to_nearest_amyloid\n",
    "    Returns:\n",
    "        dict with keys: 'total', 'counts', 'plin2', 'oro', 'lipid_droplet', 'amyloid'\n",
    "    \"\"\"\n",
    "\n",
    "    losses = {}\n",
    "\n",
    "    # NB counts loss\n",
    "    counts_loss = nb_loss(outputs[\"nb_logits\"], outputs[\"dispersion\"], batch[\"counts\"])\n",
    "    losses['counts'] = counts_loss\n",
    "\n",
    "    # Hurdle normal losses (log1p inputs)\n",
    "    plin2_loss = hurdle_normal_loss(outputs[\"plin2_logits\"], outputs[\"plin2_mean\"], outputs[\"plin2_std\"], batch[\"plin2\"])\n",
    "    losses['plin2'] = plin2_loss\n",
    "\n",
    "    oro_loss = hurdle_normal_loss(outputs[\"oro_logits\"], outputs[\"oro_mean\"], outputs[\"oro_std\"], batch[\"oro\"])\n",
    "    losses['oro'] = oro_loss\n",
    "\n",
    "    lipid_loss = hurdle_normal_loss(outputs[\"lipid_logits\"], outputs[\"lipid_mean\"], outputs[\"lipid_std\"], batch[\"lipid_droplet\"])\n",
    "    losses['lipid_droplet'] = lipid_loss\n",
    "\n",
    "    # Amyloid binary cross-entropy loss\n",
    "    amyloid_loss = amyloid_bce_loss(outputs[\"amyloid_dist_pred\"], batch[\"distance_to_nearest_amyloid\"])\n",
    "    losses['amyloid'] = amyloid_loss\n",
    "\n",
    "    # Total loss is sum of all\n",
    "    losses['total'] = sum(losses.values())\n",
    "\n",
    "    return losses\n",
    "\n",
    "def total_loss(outputs, batch, cls_emb, node_embs, corrupted_cls_emb, corrupted_node_embs, discriminator, dgi_weight=1.0):\n",
    "    \"\"\"\n",
    "    outputs: dict from MultiTaskOutputHead.forward()\n",
    "    batch: ground truth dict\n",
    "    cls_emb: [D] tensor, original graph summary ([CLS] token)\n",
    "    node_embs: [N, D] tensor, original node embeddings (all tokens except [CLS])\n",
    "    corrupted_cls_emb: [D] tensor, corrupted graph summary\n",
    "    corrupted_node_embs: [N, D] tensor, corrupted node embeddings\n",
    "    discriminator: DGIDiscriminator instance\n",
    "    dgi_weight: float, weighting factor for DGI loss\n",
    "    \"\"\"\n",
    "\n",
    "    losses = {}\n",
    "\n",
    "    # Supervised losses (same as before)\n",
    "    counts_loss = nb_loss(outputs[\"nb_logits\"], outputs[\"dispersion\"], batch[\"counts\"])\n",
    "    plin2_loss = hurdle_normal_loss(outputs[\"plin2_logits\"], outputs[\"plin2_mean\"], outputs[\"plin2_std\"], batch[\"plin2\"])\n",
    "    oro_loss = hurdle_normal_loss(outputs[\"oro_logits\"], outputs[\"oro_mean\"], outputs[\"oro_std\"], batch[\"oro\"])\n",
    "    lipid_loss = hurdle_normal_loss(outputs[\"lipid_logits\"], outputs[\"lipid_mean\"], outputs[\"lipid_std\"], batch[\"lipid_droplet\"])\n",
    "    amyloid_loss = amyloid_bce_loss(outputs[\"amyloid_dist_pred\"], batch[\"distance_to_nearest_amyloid\"])\n",
    "\n",
    "    losses.update({\n",
    "        'counts': counts_loss,\n",
    "        'plin2': plin2_loss,\n",
    "        'oro': oro_loss,\n",
    "        'lipid_droplet': lipid_loss,\n",
    "        'amyloid': amyloid_loss,\n",
    "    })\n",
    "\n",
    "    # DGI loss\n",
    "    dgi_loss_val = dgi_loss(discriminator, cls_emb, node_embs, corrupted_cls_emb, corrupted_node_embs)\n",
    "    losses['dgi'] = dgi_loss_val\n",
    "\n",
    "    # Total loss = supervised + weighted DGI\n",
    "    losses['total'] = sum([v for k, v in losses.items() if k != 'total']) + dgi_weight * dgi_loss_val\n",
    "\n",
    "    return losses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9ca3a2d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import torch\n",
    "\n",
    "def corrupt_batch(batch):\n",
    "    corrupted = copy.deepcopy(batch)  # clone so you don't overwrite original batch\n",
    "\n",
    "    # Shuffle node features along node dimension\n",
    "    perm = torch.randperm(corrupted.x.size(0))\n",
    "    corrupted.x = corrupted.x[perm]\n",
    "\n",
    "    # If you have other node attributes used by tokenizer, shuffle those similarly:\n",
    "    for attr_name in ['plin2', 'oro', 'lipid_droplet', 'distance_to_nearest_amyloid', 'counts']:\n",
    "        if hasattr(corrupted, attr_name):\n",
    "            attr = getattr(corrupted, attr_name)\n",
    "            if isinstance(attr, torch.Tensor) and attr.size(0) == corrupted.x.size(0):\n",
    "                setattr(corrupted, attr_name, attr[perm])\n",
    "\n",
    "    return corrupted\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "62a3db53",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'batch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 30\u001b[39m\n\u001b[32m     27\u001b[39m node_embs = encoded_tokens[:-\u001b[32m1\u001b[39m]        \u001b[38;5;66;03m# [N, D], all other tokens\u001b[39;00m\n\u001b[32m     29\u001b[39m \u001b[38;5;66;03m# Create corrupted input, run through tokenizer + transformer to get corrupted embeddings\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m30\u001b[39m corrupted_batch = corrupt_batch(\u001b[43mbatch\u001b[49m)  \u001b[38;5;66;03m# You define this (e.g., shuffle node features)\u001b[39;00m\n\u001b[32m     31\u001b[39m corrupted_tokens, _, _ = tokenizer(corrupted_batch)\n\u001b[32m     32\u001b[39m corrupted_tokens = corrupted_tokens.unsqueeze(\u001b[32m1\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'batch' is not defined"
     ]
    }
   ],
   "source": [
    "# Assume batch is the full subgraph data object\n",
    "\n",
    "tokens, type_ids, center_idx = tokenizer(sample)\n",
    "encoded_tokens = transformer_stack(tokens.unsqueeze(1))  # [T, 1, D]\n",
    "encoded_tokens = encoded_tokens.squeeze(1)        # [T, D]\n",
    "\n",
    "center_embedding = encoded_tokens[center_idx]     # [D]\n",
    "outputs = mtoh(center_embedding.unsqueeze(0))  # [1, D]\n",
    "\n",
    "# Extract ground truth for the center node:\n",
    "center_counts = sample.counts[center_idx].unsqueeze(0)  # [1, num_counts]\n",
    "center_plin2 = sample.plin2[center_idx].unsqueeze(0)    # [1]\n",
    "center_oro = sample.oro[center_idx].unsqueeze(0)        # [1]\n",
    "center_lipid = sample.lipid_droplet[center_idx].unsqueeze(0)  # [1]\n",
    "center_amyloid = sample.distance_to_nearest_amyloid[center_idx].unsqueeze(0)  # [1]\n",
    "\n",
    "# Make a mini \"center\" batch dict for losses\n",
    "center_batch = {\n",
    "    \"counts\": center_counts,\n",
    "    \"plin2\": center_plin2,\n",
    "    \"oro\": center_oro,\n",
    "    \"lipid_droplet\": center_lipid,\n",
    "    \"distance_to_nearest_amyloid\": center_amyloid\n",
    "}\n",
    "\n",
    "cls_emb = encoded_tokens[-1]           # [D], the [CLS] token\n",
    "node_embs = encoded_tokens[:-1]        # [N, D], all other tokens\n",
    "\n",
    "# Create corrupted input, run through tokenizer + transformer to get corrupted embeddings\n",
    "corrupted_batch = corrupt_batch(batch)  # You define this (e.g., shuffle node features)\n",
    "corrupted_tokens, _, _ = tokenizer(corrupted_batch)\n",
    "corrupted_tokens = corrupted_tokens.unsqueeze(1)\n",
    "corrupted_encoded = transformer(corrupted_tokens).squeeze(1)\n",
    "corrupted_cls_emb = corrupted_encoded[-1]\n",
    "corrupted_node_embs = corrupted_encoded[:-1]\n",
    "\n",
    "outputs = output_head(cls_emb.unsqueeze(0))\n",
    "losses = total_loss(outputs, center_batch, cls_emb, node_embs, corrupted_cls_emb, corrupted_node_embs, discriminator, dgi_weight=1.0)\n",
    "\n",
    "loss = losses['total']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d670e35",
   "metadata": {},
   "source": [
    "# Everything "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0042c3bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing previous runs because reinit is set to 'default'."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">kanna-chameleon</strong> at: <a href='https://wandb.ai/kibr/TokenGT-training/runs/kfnvngyf' target=\"_blank\">https://wandb.ai/kibr/TokenGT-training/runs/kfnvngyf</a><br> View project at: <a href='https://wandb.ai/kibr/TokenGT-training' target=\"_blank\">https://wandb.ai/kibr/TokenGT-training</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250711_121430-kfnvngyf/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.21.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/jhaberbe/Projects/Personal/TokenGT/notebook/wandb/run-20250711_121443-x5dstiuu</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/kibr/TokenGT-training/runs/x5dstiuu' target=\"_blank\">kanna-chameleon</a></strong> to <a href='https://wandb.ai/kibr/TokenGT-training' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/kibr/TokenGT-training' target=\"_blank\">https://wandb.ai/kibr/TokenGT-training</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/kibr/TokenGT-training/runs/x5dstiuu' target=\"_blank\">https://wandb.ai/kibr/TokenGT-training/runs/x5dstiuu</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jhaberbe/Projects/Personal/TokenGT/.venv/lib/python3.13/site-packages/torch/nn/modules/transformer.py:382: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(\n",
      "/home/jhaberbe/Projects/Personal/TokenGT/.venv/lib/python3.13/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n",
      "100%|██████████| 69552/69552 [13:50<00:00, 83.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | Average Loss: 8.8571\n",
      "Saved checkpoint: checkpoints/model_epoch_1.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|█▉        | 13067/69552 [02:39<11:30, 81.85it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 62\u001b[39m\n\u001b[32m     60\u001b[39m tokens, type_ids, center_idx = tokenizer(batch)\n\u001b[32m     61\u001b[39m tokens = tokens.unsqueeze(\u001b[32m1\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m62\u001b[39m encoded_tokens = \u001b[43mtransformer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokens\u001b[49m\u001b[43m)\u001b[49m.squeeze(\u001b[32m1\u001b[39m)\n\u001b[32m     64\u001b[39m center_embedding = encoded_tokens[center_idx]\n\u001b[32m     65\u001b[39m outputs = output_head(center_embedding.unsqueeze(\u001b[32m0\u001b[39m))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/Personal/TokenGT/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/Personal/TokenGT/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 19\u001b[39m, in \u001b[36mTransformerStack.forward\u001b[39m\u001b[34m(self, x, src_key_padding_mask)\u001b[39m\n\u001b[32m     16\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m x.dim() == \u001b[32m3\u001b[39m:\n\u001b[32m     17\u001b[39m     x = x.transpose(\u001b[32m0\u001b[39m, \u001b[32m1\u001b[39m)  \u001b[38;5;66;03m# to [T, B, D]\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m out = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtransformer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc_key_padding_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43msrc_key_padding_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     21\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m out.transpose(\u001b[32m0\u001b[39m, \u001b[32m1\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/Personal/TokenGT/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/Personal/TokenGT/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/Personal/TokenGT/.venv/lib/python3.13/site-packages/torch/nn/modules/transformer.py:514\u001b[39m, in \u001b[36mTransformerEncoder.forward\u001b[39m\u001b[34m(self, src, mask, src_key_padding_mask, is_causal)\u001b[39m\n\u001b[32m    511\u001b[39m is_causal = _detect_is_causal_mask(mask, is_causal, seq_len)\n\u001b[32m    513\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m mod \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.layers:\n\u001b[32m--> \u001b[39m\u001b[32m514\u001b[39m     output = \u001b[43mmod\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    515\u001b[39m \u001b[43m        \u001b[49m\u001b[43moutput\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    516\u001b[39m \u001b[43m        \u001b[49m\u001b[43msrc_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    517\u001b[39m \u001b[43m        \u001b[49m\u001b[43mis_causal\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_causal\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    518\u001b[39m \u001b[43m        \u001b[49m\u001b[43msrc_key_padding_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43msrc_key_padding_mask_for_layers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    519\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    521\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m convert_to_nested:\n\u001b[32m    522\u001b[39m     output = output.to_padded_tensor(\u001b[32m0.0\u001b[39m, src.size())\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/Personal/TokenGT/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/Personal/TokenGT/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/Personal/TokenGT/.venv/lib/python3.13/site-packages/torch/nn/modules/transformer.py:916\u001b[39m, in \u001b[36mTransformerEncoderLayer.forward\u001b[39m\u001b[34m(self, src, src_mask, src_key_padding_mask, is_causal)\u001b[39m\n\u001b[32m    911\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    912\u001b[39m     x = \u001b[38;5;28mself\u001b[39m.norm1(\n\u001b[32m    913\u001b[39m         x\n\u001b[32m    914\u001b[39m         + \u001b[38;5;28mself\u001b[39m._sa_block(x, src_mask, src_key_padding_mask, is_causal=is_causal)\n\u001b[32m    915\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m916\u001b[39m     x = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mnorm2\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_ff_block\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    918\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/Personal/TokenGT/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/Personal/TokenGT/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1755\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1751\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._call_impl(*args, **kwargs)\n\u001b[32m   1753\u001b[39m \u001b[38;5;66;03m# torchrec tests the code consistency with the following code\u001b[39;00m\n\u001b[32m   1754\u001b[39m \u001b[38;5;66;03m# fmt: off\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1755\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_call_impl\u001b[39m(\u001b[38;5;28mself\u001b[39m, *args, **kwargs):\n\u001b[32m   1756\u001b[39m     forward_call = (\u001b[38;5;28mself\u001b[39m._slow_forward \u001b[38;5;28;01mif\u001b[39;00m torch._C._get_tracing_state() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m.forward)\n\u001b[32m   1757\u001b[39m     \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m     \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in callback <bound method _WandbInit._post_run_cell_hook of <wandb.sdk.wandb_init._WandbInit object at 0x728f4ea50a50>> (for post_run_cell), with arguments args (<ExecutionResult object at 728f4eab4360, execution_count=20 error_before_exec=None error_in_exec= info=<ExecutionInfo object at 728f4ea732d0, raw_cell=\"import os\n",
      "import wandb\n",
      "import torch\n",
      "import torch.n..\" transformed_cell=\"import os\n",
      "import wandb\n",
      "import torch\n",
      "import torch.n..\" store_history=True silent=False shell_futures=True cell_id=vscode-notebook-cell:/home/jhaberbe/Projects/Personal/TokenGT/notebook/data-loading.ipynb#Y134sZmlsZQ%3D%3D> result=None>,),kwargs {}:\n"
     ]
    },
    {
     "ename": "BrokenPipeError",
     "evalue": "[Errno 32] Broken pipe",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mBrokenPipeError\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/Personal/TokenGT/.venv/lib/python3.13/site-packages/wandb/sdk/wandb_init.py:593\u001b[39m, in \u001b[36m_WandbInit._post_run_cell_hook\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    590\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m    592\u001b[39m \u001b[38;5;28mself\u001b[39m._logger.info(\u001b[33m\"\u001b[39m\u001b[33mresuming backend\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m593\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbackend\u001b[49m\u001b[43m.\u001b[49m\u001b[43minterface\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpublish_resume\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/Personal/TokenGT/.venv/lib/python3.13/site-packages/wandb/sdk/interface/interface.py:787\u001b[39m, in \u001b[36mInterfaceBase.publish_resume\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    785\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpublish_resume\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    786\u001b[39m     resume = pb.ResumeRequest()\n\u001b[32m--> \u001b[39m\u001b[32m787\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_publish_resume\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresume\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/Personal/TokenGT/.venv/lib/python3.13/site-packages/wandb/sdk/interface/interface_shared.py:294\u001b[39m, in \u001b[36mInterfaceShared._publish_resume\u001b[39m\u001b[34m(self, resume)\u001b[39m\n\u001b[32m    292\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_publish_resume\u001b[39m(\u001b[38;5;28mself\u001b[39m, resume: pb.ResumeRequest) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    293\u001b[39m     rec = \u001b[38;5;28mself\u001b[39m._make_request(resume=resume)\n\u001b[32m--> \u001b[39m\u001b[32m294\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_publish\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrec\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/Personal/TokenGT/.venv/lib/python3.13/site-packages/wandb/sdk/interface/interface_sock.py:39\u001b[39m, in \u001b[36mInterfaceSock._publish\u001b[39m\u001b[34m(self, record, local)\u001b[39m\n\u001b[32m     37\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_publish\u001b[39m(\u001b[38;5;28mself\u001b[39m, record: \u001b[33m\"\u001b[39m\u001b[33mpb.Record\u001b[39m\u001b[33m\"\u001b[39m, local: Optional[\u001b[38;5;28mbool\u001b[39m] = \u001b[38;5;28;01mNone\u001b[39;00m) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     38\u001b[39m     \u001b[38;5;28mself\u001b[39m._assign(record)\n\u001b[32m---> \u001b[39m\u001b[32m39\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sock_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43msend_record_publish\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrecord\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/Personal/TokenGT/.venv/lib/python3.13/site-packages/wandb/sdk/lib/sock_client.py:170\u001b[39m, in \u001b[36mSockClient.send_record_publish\u001b[39m\u001b[34m(self, record)\u001b[39m\n\u001b[32m    168\u001b[39m server_req.request_id = record.control.mailbox_slot\n\u001b[32m    169\u001b[39m server_req.record_publish.CopyFrom(record)\n\u001b[32m--> \u001b[39m\u001b[32m170\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msend_server_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mserver_req\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/Personal/TokenGT/.venv/lib/python3.13/site-packages/wandb/sdk/lib/sock_client.py:150\u001b[39m, in \u001b[36mSockClient.send_server_request\u001b[39m\u001b[34m(self, msg)\u001b[39m\n\u001b[32m    149\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34msend_server_request\u001b[39m(\u001b[38;5;28mself\u001b[39m, msg: spb.ServerRequest) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m150\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_send_message\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmsg\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/Personal/TokenGT/.venv/lib/python3.13/site-packages/wandb/sdk/lib/sock_client.py:147\u001b[39m, in \u001b[36mSockClient._send_message\u001b[39m\u001b[34m(self, msg)\u001b[39m\n\u001b[32m    145\u001b[39m header = struct.pack(\u001b[33m\"\u001b[39m\u001b[33m<BI\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28mord\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mW\u001b[39m\u001b[33m\"\u001b[39m), raw_size)\n\u001b[32m    146\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m._lock:\n\u001b[32m--> \u001b[39m\u001b[32m147\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sendall_with_error_handle\u001b[49m\u001b[43m(\u001b[49m\u001b[43mheader\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/Personal/TokenGT/.venv/lib/python3.13/site-packages/wandb/sdk/lib/sock_client.py:126\u001b[39m, in \u001b[36mSockClient._sendall_with_error_handle\u001b[39m\u001b[34m(self, data)\u001b[39m\n\u001b[32m    124\u001b[39m start_time = time.monotonic()\n\u001b[32m    125\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m126\u001b[39m     sent = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sock\u001b[49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    127\u001b[39m     \u001b[38;5;66;03m# sent equal to 0 indicates a closed socket\u001b[39;00m\n\u001b[32m    128\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m sent == \u001b[32m0\u001b[39m:\n",
      "\u001b[31mBrokenPipeError\u001b[39m: [Errno 32] Broken pipe"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import wandb\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim  # ← this is the missing line\n",
    "from collections import deque\n",
    "from torch_geometric.data import DataLoader\n",
    "\n",
    "# Initialize wandb project\n",
    "wandb.init(project=\"TokenGT-training\", name=\"kanna-chameleon\", config={\n",
    "    \"lr\": 1e-3,\n",
    "    \"batch_size\": 1,\n",
    "    \"epochs\": 10,\n",
    "})\n",
    "\n",
    "rolling_window_size = 100\n",
    "rolling_losses = deque(maxlen=rolling_window_size)\n",
    "rolling_component_losses = {\n",
    "    \"counts\": deque(maxlen=rolling_window_size),\n",
    "    \"plin2\": deque(maxlen=rolling_window_size),\n",
    "    \"oro\": deque(maxlen=rolling_window_size),\n",
    "    \"lipid_droplet\": deque(maxlen=rolling_window_size),\n",
    "    \"amyloid\": deque(maxlen=rolling_window_size),\n",
    "    \"dgi\": deque(maxlen=rolling_window_size),\n",
    "}\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "discriminator = DGIDiscriminator(256).to(device)\n",
    "\n",
    "# Model components\n",
    "tokenizer = GraphTokenizer(input_dim=sample.x.shape[1], hidden_dim=256).to(device)\n",
    "transformer = TransformerStack(d_model=256, num_heads=8, num_layers=4).to(device)\n",
    "output_head = MultiTaskOutputHead(embedding_dim=256, num_counts=366).to(device)\n",
    "\n",
    "# Optimizer\n",
    "params = list(tokenizer.parameters()) + list(transformer.parameters()) + list(output_head.parameters()) + list(discriminator.parameters())\n",
    "optimizer = optim.Adam(params, lr=1e-5)\n",
    "\n",
    "# Data loader\n",
    "loader = DataLoader(dataset, batch_size=1, shuffle=True)\n",
    "\n",
    "# Directory for checkpoints\n",
    "checkpoint_dir = \"checkpoints\"\n",
    "os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "\n",
    "for epoch in range(wandb.config.epochs):\n",
    "    total = 0.0\n",
    "    total_component_loss = {k: 0.0 for k in rolling_component_losses.keys()}\n",
    "\n",
    "    tokenizer.train()\n",
    "    transformer.train()\n",
    "    output_head.train()\n",
    "    discriminator.train()\n",
    "\n",
    "    batch_num = 0\n",
    "    for batch in tqdm(loader):\n",
    "        batch_num += 1\n",
    "        batch = batch.to(device)\n",
    "\n",
    "        tokens, type_ids, center_idx = tokenizer(batch)\n",
    "        tokens = tokens.unsqueeze(1)\n",
    "        encoded_tokens = transformer(tokens).squeeze(1)\n",
    "\n",
    "        center_embedding = encoded_tokens[center_idx]\n",
    "        outputs = output_head(center_embedding.unsqueeze(0))\n",
    "\n",
    "        center_counts = batch.counts[center_idx].unsqueeze(0).to(device)\n",
    "        center_plin2 = batch.plin2[center_idx].unsqueeze(0).to(device)\n",
    "        center_oro = batch.oro[center_idx].unsqueeze(0).to(device)\n",
    "        center_lipid = batch.lipid_droplet[center_idx].unsqueeze(0).to(device)\n",
    "        center_amyloid = batch.distance_to_nearest_amyloid[center_idx].unsqueeze(0).to(device)\n",
    "\n",
    "        center_batch = {\n",
    "            \"counts\": center_counts,\n",
    "            \"plin2\": center_plin2,\n",
    "            \"oro\": center_oro,\n",
    "            \"lipid_droplet\": center_lipid,\n",
    "            \"distance_to_nearest_amyloid\": center_amyloid\n",
    "        }\n",
    "\n",
    "        cls_emb = encoded_tokens[-1]           # [D], the [CLS] token\n",
    "        node_embs = encoded_tokens[:-1]        # [N, D], all other tokens\n",
    "\n",
    "        corrupted_batch = corrupt_batch(batch).to(device)\n",
    "        corrupted_tokens, _, _ = tokenizer(corrupted_batch)\n",
    "        corrupted_tokens = corrupted_tokens.unsqueeze(1)\n",
    "        corrupted_encoded = transformer(corrupted_tokens).squeeze(1)\n",
    "        corrupted_cls_emb = corrupted_encoded[-1]\n",
    "        corrupted_node_embs = corrupted_encoded[:-1]\n",
    "\n",
    "        outputs = output_head(cls_emb.unsqueeze(0))\n",
    "        loss_dict = total_loss(outputs, center_batch, cls_emb, node_embs, corrupted_cls_emb, corrupted_node_embs, discriminator, dgi_weight=1.0)\n",
    "        loss = loss_dict['total']\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total += loss.item()\n",
    "        for k in rolling_component_losses.keys():\n",
    "            total_component_loss[k] += loss_dict.get(k, 0).item()\n",
    "\n",
    "        rolling_losses.append(loss.item())\n",
    "        for k in rolling_component_losses.keys():\n",
    "            rolling_component_losses[k].append(loss_dict.get(k, 0).item())\n",
    "\n",
    "        if batch_num % 10 == 0:\n",
    "            wandb.log({\n",
    "                \"rolling_loss\": sum(rolling_losses) / len(rolling_losses),\n",
    "                **{f\"rolling_{k}_loss\": sum(v) / len(v) for k, v in rolling_component_losses.items()},\n",
    "                \"batch\": batch_num,\n",
    "                \"epoch\": epoch + 1\n",
    "            })\n",
    "\n",
    "    # Log epoch average losses\n",
    "    epoch_loss = total / len(loader)\n",
    "    wandb.log({\n",
    "        \"epoch_loss\": epoch_loss,\n",
    "        **{f\"epoch_{k}_loss\": total_component_loss[k] / len(loader) for k in rolling_component_losses},\n",
    "        \"epoch\": epoch + 1,\n",
    "    })\n",
    "\n",
    "    print(f\"Epoch {epoch+1} | Average Loss: {epoch_loss:.4f}\")\n",
    "\n",
    "    # Save checkpoint\n",
    "    checkpoint_path = os.path.join(checkpoint_dir, f\"model_epoch_{epoch+1}.pt\")\n",
    "    torch.save({\n",
    "        \"epoch\": epoch + 1,\n",
    "        \"tokenizer_state_dict\": tokenizer.state_dict(),\n",
    "        \"transformer_state_dict\": transformer.state_dict(),\n",
    "        \"output_head_state_dict\": output_head.state_dict(),\n",
    "        \"discriminator_state_dict\": discriminator.state_dict(),\n",
    "        \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "        \"loss\": epoch_loss,\n",
    "    }, checkpoint_path)\n",
    "    print(f\"Saved checkpoint: {checkpoint_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "917cca3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = torch.load(checkpoint_path)\n",
    "tokenizer.load_state_dict(checkpoint[\"tokenizer_state_dict\"])\n",
    "transformer.load_state_dict(checkpoint[\"transformer_state_dict\"])\n",
    "output_head.load_state_dict(checkpoint[\"output_head_state_dict\"])\n",
    "discriminator.load_state_dict(checkpoint[\"discriminator_state_dict\"])\n",
    "optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b690740",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from torch_geometric.data import DataLoader\n",
    "\n",
    "# Paths\n",
    "checkpoint_dir = \"checkpoints\"\n",
    "output_dir = \"embeddings\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Choose which checkpoint to use\n",
    "checkpoint_path = os.path.join(checkpoint_dir, \"model_epoch_1.pt\")\n",
    "\n",
    "# Load model components\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "tokenizer = GraphTokenizer(input_dim=sample.x.shape[1], hidden_dim=256).to(device)\n",
    "transformer = TransformerStack(d_model=256, num_heads=8, num_layers=4).to(device)\n",
    "\n",
    "# Load saved weights\n",
    "checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "tokenizer.load_state_dict(checkpoint[\"tokenizer_state_dict\"])\n",
    "transformer.load_state_dict(checkpoint[\"transformer_state_dict\"])\n",
    "\n",
    "tokenizer.eval()\n",
    "transformer.eval()\n",
    "\n",
    "# Set up data loader\n",
    "loader = DataLoader(dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "cls_embeddings = []\n",
    "center_embeddings = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(loader):\n",
    "        batch = batch.to(device)\n",
    "        \n",
    "        tokens, _, center_idx = tokenizer(batch)\n",
    "        tokens = tokens.unsqueeze(1)  # Add sequence dimension\n",
    "        encoded = transformer(tokens).squeeze(1)  # Remove sequence dimension\n",
    "\n",
    "        cls_emb = encoded[-1].detach().cpu().numpy()             # [D]\n",
    "        center_emb = encoded[center_idx].detach().cpu().numpy()  # [D]\n",
    "\n",
    "        cls_embeddings.append(cls_emb)\n",
    "        center_embeddings.append(center_emb)\n",
    "\n",
    "# Convert to arrays and save\n",
    "cls_embeddings = np.stack(cls_embeddings)\n",
    "center_embeddings = np.stack(center_embeddings)\n",
    "\n",
    "np.save(os.path.join(output_dir, \"cls_embeddings.npy\"), cls_embeddings)\n",
    "np.save(os.path.join(output_dir, \"center_embeddings.npy\"), center_embeddings)\n",
    "\n",
    "print(\"Saved embeddings:\")\n",
    "print(f\" - [CLS]:   {cls_embeddings.shape}\")\n",
    "print(f\" - [Center]: {center_embeddings.shape}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
